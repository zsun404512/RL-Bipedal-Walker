{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC) for BipedalWalker-v3\n",
    "\n",
    "This notebook implements a Soft Actor-Critic (SAC) agent to solve the BipedalWalker-v3 environment from Gymnasium.\n",
    "SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install gymnasium[box2d]\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import cv2  # Added for parallel window rendering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Change to standard tqdm to avoid notebook widget errors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# For local training on Mac, \n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        torch.cuda.set_device(0)\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"CUDA GPU: {gpu_name} | Memory: {props.total_memory/1024**3:.1f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU info not available: {e}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer stores experience tuples (state, action, reward, next_state, done) to be sampled during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, buffer_size=int(1e6)):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.state = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
    "        self.action = np.zeros((buffer_size, action_dim), dtype=np.float32)\n",
    "        self.reward = np.zeros((buffer_size, 1), dtype=np.float32)\n",
    "        self.next_state = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
    "        self.done = np.zeros((buffer_size, 1), dtype=np.float32)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Check if input is batch or single\n",
    "        if state.ndim == 1:\n",
    "            state = state[None, :]\n",
    "            action = action[None, :]\n",
    "            reward = np.array(reward)[None]\n",
    "            next_state = next_state[None, :]\n",
    "            done = np.array(done)[None]\n",
    "        \n",
    "        batch_size = len(state)\n",
    "        \n",
    "        if self.ptr + batch_size <= self.buffer_size:\n",
    "            self.state[self.ptr:self.ptr+batch_size] = state\n",
    "            self.action[self.ptr:self.ptr+batch_size] = action\n",
    "            self.reward[self.ptr:self.ptr+batch_size] = reward.reshape(-1, 1)\n",
    "            self.next_state[self.ptr:self.ptr+batch_size] = next_state\n",
    "            self.done[self.ptr:self.ptr+batch_size] = done.reshape(-1, 1)\n",
    "            self.ptr = (self.ptr + batch_size) % self.buffer_size\n",
    "        else:\n",
    "            # Handle wrap around\n",
    "            overflow = (self.ptr + batch_size) - self.buffer_size\n",
    "            split = batch_size - overflow\n",
    "            \n",
    "            # First part\n",
    "            self.state[self.ptr:] = state[:split]\n",
    "            self.action[self.ptr:] = action[:split]\n",
    "            self.reward[self.ptr:] = reward[:split].reshape(-1, 1)\n",
    "            self.next_state[self.ptr:] = next_state[:split]\n",
    "            self.done[self.ptr:] = done[:split].reshape(-1, 1)\n",
    "            \n",
    "            # Second part (overflow)\n",
    "            self.state[:overflow] = state[split:]\n",
    "            self.action[:overflow] = action[split:]\n",
    "            self.reward[:overflow] = reward[split:].reshape(-1, 1)\n",
    "            self.next_state[:overflow] = next_state[split:]\n",
    "            self.done[:overflow] = done[split:].reshape(-1, 1)\n",
    "            self.ptr = overflow\n",
    "            \n",
    "        self.size = min(self.size + batch_size, self.buffer_size)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            self.state[ind],\n",
    "            self.action[ind],\n",
    "            self.reward[ind],\n",
    "            self.next_state[ind],\n",
    "            self.done[ind]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architectures\n",
    "\n",
    "We define the Actor and Critic networks. The Actor outputs the mean and log standard deviation of the action distribution. The Critic estimates the Q-value for a given state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, action_dim)\n",
    "        # Output log_std from the network instead of a fixed parameter\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        # Constrain log_std for numerical stability\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def sample(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        x_t = dist.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        \n",
    "        # Log prob calculation\n",
    "        log_prob = dist.log_prob(x_t)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        # Critic takes state and action as input\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q_value = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.q_value(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC Agent\n",
    "\n",
    "The SAC agent orchestrates the interaction with the environment and the training process. It maintains the actor, two critics (for double Q-learning), and their target networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, action_scale=1.0, device=\"cpu\", learning_rate=3e-4):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_scale = action_scale\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.alpha = 0.2\n",
    "        self.batch_size = 256\n",
    "        self.buffer_size = int(1e6)\n",
    "        # Automatic entropy tuning target: -dim(A)\n",
    "        self.target_entropy = -float(action_dim)\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = ActorNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.critic1 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.critic2 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_critic1 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_critic2 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
    "        \n",
    "        # Copy weights to target networks\n",
    "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.learning_rate)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Replay buffer (Now using optimized Numpy buffer with batch support)\n",
    "        self.replay_buffer = ReplayBuffer(state_dim, action_dim, self.buffer_size)\n",
    "        \n",
    "        # Log alpha for entropy adjustment\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.learning_rate)\n",
    "        \n",
    "    def select_action(self, state, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            if state.ndim == 1:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            else:\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                \n",
    "            if deterministic:\n",
    "                mu, _ = self.actor(state)\n",
    "                action = torch.tanh(mu)\n",
    "                return action.cpu().numpy() # Return batch if input was batch\n",
    "            \n",
    "            action, _ = self.actor.sample(state)\n",
    "            return action.cpu().numpy()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "            \n",
    "        # Sample from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_pi = self.actor.sample(next_states)\n",
    "            next_q1 = self.target_critic1(next_states, next_actions)\n",
    "            next_q2 = self.target_critic2(next_states, next_actions)\n",
    "            next_q = torch.min(next_q1, next_q2) - self.alpha * next_log_pi\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        current_q1 = self.critic1(states, actions)\n",
    "        current_q2 = self.critic2(states, actions)\n",
    "        \n",
    "        critic1_loss = F.mse_loss(current_q1, target_q)\n",
    "        critic2_loss = F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        self.critic1_optimizer.step()\n",
    "        \n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        actions_pred, log_pi = self.actor.sample(states)\n",
    "        q1 = self.critic1(states, actions_pred)\n",
    "        q2 = self.critic2(states, actions_pred)\n",
    "        q = torch.min(q1, q2)\n",
    "        \n",
    "        actor_loss = (self.alpha * log_pi - q).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Failsafe for target_entropy\n",
    "        if not hasattr(self, 'target_entropy'):\n",
    "            self.target_entropy = -float(self.action_dim)\n",
    "            \n",
    "        # Update alpha\n",
    "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "        \n",
    "        # Update target networks\n",
    "        for param, target_param in zip(self.critic1.parameters(), self.target_critic1.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            \n",
    "        for param, target_param in zip(self.critic2.parameters(), self.target_critic2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "We train the agent in the environment. We'll also log rewards and save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a custom handler to work with tqdm\n",
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def __init__(self, level=logging.NOTSET):\n",
    "        super().__init__(level)\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "def setup_logger(log_dir=\"logs\"):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    logger = logging.getLogger(\"BipedalWalker\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Clear existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "        \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = os.path.join(log_dir, f\"training_{timestamp}.log\")\n",
    "    \n",
    "    # File handler\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    \n",
    "    # Console handler using TqdmLoggingHandler\n",
    "    ch = TqdmLoggingHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class WalkingRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def step(self, action):\n",
    "        state, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # Original reward already includes forward progress and fall penalties.\n",
    "        # We add a small penalty for 'torque' (action magnitude) to encourage efficiency.\n",
    "        # Action is usually in range [-1, 1].\n",
    "        energy_penalty = -0.001 * np.sum(np.square(action))\n",
    "        \n",
    "        # Reward for keeping the head (hull) stable/upright\n",
    "        # state[0] is hull angle, 0 is vertical.\n",
    "        stability_reward = -0.1 * abs(state[0])\n",
    "        \n",
    "        reward += energy_penalty + stability_reward\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "def save_checkpoint(agent, episode, learning_rate, log_dir=\"logs\", filename=None):\n",
    "    if filename is None:\n",
    "        filename = f\"checkpoint_ep{episode}.pth\"\n",
    "    \n",
    "    path = os.path.join(log_dir, filename)\n",
    "    torch.save({\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic1_state_dict': agent.critic1.state_dict(),\n",
    "        'critic2_state_dict': agent.critic2.state_dict(),\n",
    "        'episode': episode,\n",
    "        'learning_rate': learning_rate\n",
    "    }, path)\n",
    "    return path\n",
    "\n",
    "def train_agent(env_name=\"BipedalWalker-v3\", max_episodes=1000, max_steps=1000, device=\"cpu\", render_freq=50, learning_rate=3e-4, updates_per_step=1, start_steps=20000, num_envs=1, save_interval=10, log_dir=None):\n",
    "    # Determine log directory\n",
    "    if log_dir is None:\n",
    "        log_dir = os.path.join(\"logs\", datetime.now().strftime(\"run_%Y%m%d_%H%M%S\"))\n",
    "\n",
    "    # Setup logger\n",
    "    logger = setup_logger(log_dir=log_dir)\n",
    "    logger.info(f\"Starting training with device: {device}, LR: {learning_rate}, Num Envs: {num_envs}\")\n",
    "    logger.info(f\"Logs and checkpoints will be saved to: {log_dir}\")\n",
    "    \n",
    "    # Determine render mode\n",
    "    # Always use rgb_array to allow conditional rendering without re-init\n",
    "    render_mode = \"rgb_array\"\n",
    "    \n",
    "    # Track per-environment progress\n",
    "    env_step_counts = np.zeros(num_envs, dtype=int)\n",
    "    env_episode_counts = np.zeros(num_envs, dtype=int)\n",
    "    \n",
    "    # Create environment\n",
    "    if num_envs > 1:\n",
    "        # Use WalkingRewardWrapper\n",
    "        vec_mode = \"async\" # Async is usually faster\n",
    "        env = gym.make_vec(env_name, num_envs=num_envs, vectorization_mode=vec_mode, wrappers=[WalkingRewardWrapper], render_mode=render_mode)\n",
    "        logger.info(f\"Using {num_envs} vectorized environments ({vec_mode}) with WalkingRewardWrapper\")\n",
    "    else:\n",
    "        env = gym.make(env_name, render_mode=render_mode)\n",
    "        env = WalkingRewardWrapper(env)\n",
    "        logger.info(f\"Using WalkingRewardWrapper\")\n",
    "        \n",
    "    if num_envs > 1:\n",
    "        state_dim = env.single_observation_space.shape[0]\n",
    "        action_dim = env.single_action_space.shape[0]\n",
    "        action_scale = float(env.single_action_space.high[0])\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        action_scale = float(env.action_space.high[0])\n",
    "    \n",
    "    # Initialize agent\n",
    "    logger.info(f\"Initializing SAC Agent on device: {device}\")\n",
    "    agent = SACAgent(state_dim, action_dim, action_scale, device=device, learning_rate=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    total_steps = 0\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(range(max_episodes), desc=f\"Training Progress\", unit=\"ep\")\n",
    "    \n",
    "    current_episode = 0\n",
    "    \n",
    "    # Reset env\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    try:\n",
    "        while current_episode < max_episodes:\n",
    "            episode_reward = 0 \n",
    "            if num_envs > 1:\n",
    "                current_rewards = np.zeros(num_envs)\n",
    "                \n",
    "            # Determine if we should render this episode\n",
    "            should_render = (render_freq > 0) and (current_episode % render_freq == 0)\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                # Select action\n",
    "                if total_steps < start_steps:\n",
    "                    if num_envs > 1:\n",
    "                        action = np.array([env.single_action_space.sample() for _ in range(num_envs)])\n",
    "                    else:\n",
    "                        action = env.action_space.sample()\n",
    "                else:\n",
    "                    if num_envs > 1:\n",
    "                        action = agent.select_action(state, deterministic=False) \n",
    "                    else:\n",
    "                        action = agent.select_action(state)\n",
    "                \n",
    "                # Take step\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                \n",
    "                # Rendering logic\n",
    "                if should_render:\n",
    "                    try:\n",
    "                        frames = env.render()\n",
    "                        # Display frames logic\n",
    "                        if isinstance(frames, (tuple, list)) and len(frames) == num_envs:\n",
    "                             # Render only the first environment\n",
    "                             frame = frames[0]\n",
    "                             if isinstance(frame, np.ndarray):\n",
    "                                bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                                cv2.putText(bgr_frame, f\"Ep: {current_episode}\", (10, 30), \n",
    "                                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                                cv2.imshow(f\"Training Preview\", bgr_frame)\n",
    "                                cv2.waitKey(1)\n",
    "                        elif isinstance(frames, np.ndarray):\n",
    "                             bgr_frame = cv2.cvtColor(frames, cv2.COLOR_RGB2BGR)\n",
    "                             cv2.putText(bgr_frame, f\"Ep: {current_episode}\", (10, 30), \n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                             cv2.imshow(\"Training Preview\", bgr_frame)\n",
    "                             cv2.waitKey(1)\n",
    "                    except Exception as e:\n",
    "                        # Sometimes rendering fails if window closed\n",
    "                        pass\n",
    "                \n",
    "                # Handle done/truncated\n",
    "                if num_envs > 1:\n",
    "                    done_flag = done | truncated\n",
    "                    current_rewards += reward\n",
    "                    \n",
    "                    for i in range(num_envs):\n",
    "                        if done_flag[i]:\n",
    "                            episode_rewards.append(current_rewards[i])\n",
    "                            current_rewards[i] = 0\n",
    "                            current_episode += 1\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                            # Update render status\n",
    "                            should_render = (render_freq > 0) and (current_episode % render_freq == 0)\n",
    "                            \n",
    "                            # Checkpoint logic\n",
    "                            if current_episode % save_interval == 0:\n",
    "                                path = save_checkpoint(agent, current_episode, learning_rate, log_dir=log_dir)\n",
    "                                logger.info(f\"Checkpoint saved at episode {current_episode}: {path}\")\n",
    "                                \n",
    "                            if current_episode >= max_episodes:\n",
    "                                break\n",
    "                    \n",
    "                    # Buffer addition for vec env\n",
    "                    real_next_states = next_state.copy()\n",
    "                    if \"_final_observation\" in info:\n",
    "                        mask = info[\"_final_observation\"]\n",
    "                        for i, is_final in enumerate(mask):\n",
    "                            if is_final and \"final_observation\" in info:\n",
    "                                real_next_states[i] = info[\"final_observation\"][i]\n",
    "                    \n",
    "                    agent.replay_buffer.add(state, action, reward, real_next_states, done_flag)\n",
    "                    \n",
    "                else:\n",
    "                    done_flag = done or truncated\n",
    "                    agent.replay_buffer.add(state, action, reward, next_state, done_flag)\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    if done_flag:\n",
    "                        episode_rewards.append(episode_reward)\n",
    "                        current_episode += 1\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                        should_render = (render_freq > 0) and (current_episode % render_freq == 0)\n",
    "                        \n",
    "                        # Checkpoint logic\n",
    "                        if current_episode % save_interval == 0:\n",
    "                             path = save_checkpoint(agent, current_episode, learning_rate, log_dir=log_dir)\n",
    "                             logger.info(f\"Checkpoint saved at episode {current_episode}: {path}\")\n",
    "                        \n",
    "                        state, _ = env.reset()\n",
    "                        break\n",
    "                \n",
    "                state = next_state\n",
    "                total_steps += num_envs\n",
    "                \n",
    "                # Update agent\n",
    "                if len(agent.replay_buffer) > agent.batch_size and total_steps >= start_steps:\n",
    "                    for _ in range(updates_per_step * num_envs):\n",
    "                        agent.update()\n",
    "                        \n",
    "            # Logging progress occasionally\n",
    "            if len(episode_rewards) > 0 and current_episode % 10 == 0:\n",
    "                avg_reward = np.mean(episode_rewards[-10:])\n",
    "                pbar.set_postfix({\n",
    "                    'Last Reward': f'{episode_rewards[-1]:.2f}',\n",
    "                    'Avg Reward (10)': f'{avg_reward:.2f}',\n",
    "                    'Steps': total_steps\n",
    "                })\n",
    "                logger.info(f\"Episode {current_episode}: Avg Reward (10) = {avg_reward:.2f}, Total Steps = {total_steps}\")\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Training interrupted by user. Saving emergency checkpoint...\")\n",
    "        path = save_checkpoint(agent, current_episode, learning_rate, log_dir=log_dir, filename=f\"emergency_checkpoint_ep{current_episode}.pth\")\n",
    "        logger.info(f\"Emergency checkpoint saved: {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred: {e}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        try:\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "        except:\n",
    "            pass\n",
    "        env.close()\n",
    "        logger.info(\"Training finished/stopped.\")\n",
    "\n",
    "    return episode_rewards, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Run the training loop and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BipedalWalker training on mps...\n",
      "2025-12-26 03:39:21,756 - INFO - Starting training with device: mps, LR: 0.0003, Num Envs: 4\n",
      "2025-12-26 03:39:21,756 - INFO - Logs and checkpoints will be saved to: logs/run_20251226_033921\n",
      "2025-12-26 03:39:21,758 - INFO - Using 4 vectorized environments (sync) with AlternatingLegsRewardWrapper (scale=5.0)\n",
      "2025-12-26 03:39:21,758 - INFO - Initializing SAC Agent on device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 11/1000 [00:37<1:21:25,  4.94s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:39:59,348 - INFO - Checkpoint saved at episode 10: logs/run_20251226_033921/checkpoint_ep10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▏         | 20/1000 [00:47<22:38,  1.39s/ep]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:40:09,227 - INFO - Checkpoint saved at episode 20: logs/run_20251226_033921/checkpoint_ep20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 30/1000 [02:07<1:02:06,  3.84s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:41:29,296 - INFO - Checkpoint saved at episode 30: logs/run_20251226_033921/checkpoint_ep30.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   4%|▍         | 40/1000 [02:26<22:46,  1.42s/ep]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:41:48,108 - INFO - Checkpoint saved at episode 40: logs/run_20251226_033921/checkpoint_ep40.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▌         | 50/1000 [02:53<40:20,  2.55s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:42:15,114 - INFO - Checkpoint saved at episode 50: logs/run_20251226_033921/checkpoint_ep50.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▌         | 61/1000 [03:26<32:00,  2.05s/ep]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:42:47,710 - INFO - Checkpoint saved at episode 60: logs/run_20251226_033921/checkpoint_ep60.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   7%|▋         | 70/1000 [03:51<37:09,  2.40s/ep]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:43:13,396 - INFO - Checkpoint saved at episode 70: logs/run_20251226_033921/checkpoint_ep70.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   8%|▊         | 80/1000 [04:16<32:00,  2.09s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:43:37,967 - INFO - Checkpoint saved at episode 80: logs/run_20251226_033921/checkpoint_ep80.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   9%|▉         | 90/1000 [04:58<1:13:46,  4.86s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:44:20,402 - INFO - Checkpoint saved at episode 90: logs/run_20251226_033921/checkpoint_ep90.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  10%|█         | 100/1000 [05:31<38:02,  2.54s/ep] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:44:52,820 - INFO - Checkpoint saved at episode 100: logs/run_20251226_033921/checkpoint_ep100.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  11%|█         | 110/1000 [05:57<37:29,  2.53s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:45:18,891 - INFO - Checkpoint saved at episode 110: logs/run_20251226_033921/checkpoint_ep110.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|█▏        | 120/1000 [06:25<39:46,  2.71s/ep]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:45:47,551 - INFO - Checkpoint saved at episode 120: logs/run_20251226_033921/checkpoint_ep120.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  13%|█▎        | 130/1000 [07:07<1:00:44,  4.19s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:46:29,049 - INFO - Checkpoint saved at episode 130: logs/run_20251226_033921/checkpoint_ep130.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  14%|█▍        | 140/1000 [07:54<1:05:10,  4.55s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:47:16,055 - INFO - Checkpoint saved at episode 140: logs/run_20251226_033921/checkpoint_ep140.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  15%|█▌        | 150/1000 [08:30<55:41,  3.93s/ep]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:47:52,341 - INFO - Checkpoint saved at episode 150: logs/run_20251226_033921/checkpoint_ep150.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  16%|█▌        | 160/1000 [09:09<55:05,  3.94s/ep]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:48:30,963 - INFO - Checkpoint saved at episode 160: logs/run_20251226_033921/checkpoint_ep160.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  17%|█▋        | 170/1000 [09:48<58:49,  4.25s/ep]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:49:10,151 - INFO - Checkpoint saved at episode 170: logs/run_20251226_033921/checkpoint_ep170.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  18%|█▊        | 180/1000 [10:25<58:55,  4.31s/ep]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:49:46,881 - INFO - Checkpoint saved at episode 180: logs/run_20251226_033921/checkpoint_ep180.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  19%|█▉        | 190/1000 [11:24<1:37:51,  7.25s/ep]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 03:50:46,436 - INFO - Checkpoint saved at episode 190: logs/run_20251226_033921/checkpoint_ep190.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  20%|█▉        | 196/1000 [12:06<1:34:40,  7.06s/ep]"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(f\"Starting BipedalWalker training on {device}...\")\n",
    "\n",
    "# Configurable Hyperparameters\n",
    "MAX_EPISODES = 2000 # Increased to allow more training time\n",
    "UPDATES_PER_STEP = 2  # Higher gradient updates per step for sample efficiency\n",
    "NUM_ENVS = 4 # Number of parallel environments (vectorized)\n",
    "START_STEPS = 20000 # Increased exploration\n",
    "RENDER_FREQ = 50 # Render every 50 episodes\n",
    "\n",
    "rewards, trained_agent = train_agent(\n",
    "    max_episodes=MAX_EPISODES, \n",
    "    device=device, \n",
    "    updates_per_step=UPDATES_PER_STEP,\n",
    "    start_steps=START_STEPS,\n",
    "    num_envs=NUM_ENVS,\n",
    "    render_freq=RENDER_FREQ\n",
    ") \n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Training Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Recording and Evaluation\n",
    "\n",
    "After training, we can record a video of the agent's performance to visually verify its walking ability. This is required for the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "from IPython.display import Video\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def record_video(agent, env_name=\"BipedalWalker-v3\", filename=\"bipedal_walker\", device=\"cpu\"):\n",
    "    # Create environment with render mode\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    \n",
    "    # Wrap environment to record video\n",
    "    # We force record the first episode\n",
    "    video_folder = \"videos\"\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "    env = RecordVideo(env, video_folder=video_folder, name_prefix=filename, episode_trigger=lambda x: True)\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        # Use deterministic policy for evaluation\n",
    "        action = agent.select_action(state, deterministic=True)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"Evaluation Run - Total Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    # Find the video file\n",
    "    mp4_files = glob.glob(f\"{video_folder}/{filename}-episode-0.mp4\")\n",
    "    if mp4_files:\n",
    "        print(f\"Video saved to {mp4_files[0]}\")\n",
    "        return mp4_files[0]\n",
    "    return None\n",
    "\n",
    "# Record and display video\n",
    "# Note: You need to have ffmpeg installed for this to work\n",
    "if 'trained_agent' in locals():\n",
    "    video_path = record_video(trained_agent, device=device)\n",
    "    if video_path:\n",
    "        display(Video(video_path, embed=True, html_attributes=\"controls autoplay loop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_study(device=\"cpu\"):\n",
    "    # Compare different learning rates\n",
    "    learning_rates = [1e-3, 3e-4, 1e-4]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Starting Ablation Study on Learning Rates...\")\n",
    "    \n",
    "    # Create a timestamped directory for the ablation study\n",
    "    study_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    study_dir = os.path.join(\"logs\", f\"ablation_{study_timestamp}\")\n",
    "    os.makedirs(study_dir, exist_ok=True)\n",
    "    print(f\"Ablation results will be saved to: {study_dir}\")\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTesting Learning Rate: {lr}\")\n",
    "        # Run training for fewer episodes for ablation study to save time, or full duration if desired\n",
    "        # Using 500 episodes for ablation study demonstration\n",
    "        ablation_episodes = 500 \n",
    "        \n",
    "        # Use a separate subfolder for each learning rate to avoid file conflicts\n",
    "        # and to keep logs organized\n",
    "        run_log_dir = os.path.join(study_dir, f\"lr_{lr}\")\n",
    "        \n",
    "        # render_freq=0 disables rendering for speed\n",
    "        rewards, _ = train_agent(max_episodes=ablation_episodes, device=device, learning_rate=lr, log_dir=run_log_dir, render_freq=0)\n",
    "        results[f\"lr_{lr}\"] = rewards\n",
    "        \n",
    "        # Save intermediate result in the study directory\n",
    "        np.save(os.path.join(study_dir, f\"ablation_rewards_lr_{lr}.npy\"), rewards)\n",
    "    \n",
    "    print(\"Ablation study completed. Plotting results...\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, rewards in results.items():\n",
    "        # Smooth rewards for better visualization\n",
    "        window = 10\n",
    "        smoothed_rewards = [np.mean(rewards[max(0, i-window):i+1]) for i in range(len(rewards))]\n",
    "        plt.plot(smoothed_rewards, label=name)\n",
    "        \n",
    "    plt.title(\"Ablation Study: Learning Rates (Smoothed)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Run the ablation study\n",
    "run_ablation_study(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AISF Application Writeup\n",
    "\n",
    "## Applicant Information\n",
    "- **Name**: [Your Name]\n",
    "- **Email**: [Your Email]\n",
    "\n",
    "## Prior Experience\n",
    "*Briefly describe any prior experience you have in RL, Deep Learning, and coding.*\n",
    "\n",
    "## Time Breakdown\n",
    "*Total hours spent: X hours*\n",
    "- Research/Reading: X hours\n",
    "- Coding: X hours\n",
    "- Training/Tuning: X hours\n",
    "- Writing: X hours\n",
    "\n",
    "## Compute Resources\n",
    "*Describe the hardware used.*\n",
    "- CPU: [Number of threads, Model]\n",
    "- GPU: [Model] (if applicable)\n",
    "\n",
    "## Techniques Used\n",
    "*List the techniques used (e.g., SAC, Double Q-Learning, Soft Updates) and justify why they are suitable for this environment.*\n",
    "\n",
    "## Ablation Studies\n",
    "*Summarize the results of your ablation studies here. What hyperparameters did you tweak? What was the impact?*\n",
    "\n",
    "## Discussion of Issues\n",
    "*Discuss challenges encountered, ideas that didn't work, and how you pivoted.*\n",
    "\n",
    "## Conclusion\n",
    "*Summarize what worked well and what could be improved with more time.*\n",
    "\n",
    "## Citations\n",
    "- [Haarnoja, T., et al. \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\" arXiv preprint arXiv:1801.01290 (2018).](https://arxiv.org/abs/1801.01290)\n",
    "- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
