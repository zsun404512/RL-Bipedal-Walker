{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zsun404512/RL-Bipedal-Walker/blob/main/bipedal_walker_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfbEWVfdIYv8"
      },
      "source": [
        "# Soft Actor-Critic (SAC) for BipedalWalker-v3 (trial!!!!)\n",
        "\n",
        "This notebook implements a Soft Actor-Critic (SAC) agent to solve the BipedalWalker-v3 environment from Gymnasium.\n",
        "SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idd7Fw-wIYv9",
        "outputId": "f52c3d23-eb74-4cd0-e21b-bd70804be601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.4.0-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.4.0\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.4.0)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp312-cp312-linux_x86_64.whl size=2399012 sha256=63da289747b142cdbe68813962ece55a1f1cecd84352146bcf7d207e5b2ac2d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/e9/60/774da0bcd07f7dc7761a8590fa2d065e4069568e78dcdc3318\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import cv2  # Added for parallel window rendering\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "# Change to standard tqdm to avoid notebook widget errors\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3IhEgn9IYv9",
        "outputId": "03ea54a7-6597-4d02-954c-6e147243e512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# For local training on Mac,\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    try:\n",
        "        torch.cuda.set_device(0)\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        props = torch.cuda.get_device_properties(0)\n",
        "        print(f\"CUDA GPU: {gpu_name} | Memory: {props.total_memory/1024**3:.1f} GB\")\n",
        "    except Exception as e:\n",
        "        print(f\"GPU info not available: {e}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6KERURMIYv9"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "The replay buffer stores experience tuples (state, action, reward, next_state, done) to be sampled during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FM6q0h6LIYv9"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, state_dim, action_dim, buffer_size=int(1e6)):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "        self.state = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
        "        self.action = np.zeros((buffer_size, action_dim), dtype=np.float32)\n",
        "        self.reward = np.zeros((buffer_size, 1), dtype=np.float32)\n",
        "        self.next_state = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
        "        self.done = np.zeros((buffer_size, 1), dtype=np.float32)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        # Check if input is batch or single\n",
        "        if state.ndim == 1:\n",
        "            state = state[None, :]\n",
        "            action = action[None, :]\n",
        "            reward = np.array(reward)[None]\n",
        "            next_state = next_state[None, :]\n",
        "            done = np.array(done)[None]\n",
        "\n",
        "        batch_size = len(state)\n",
        "\n",
        "        if self.ptr + batch_size <= self.buffer_size:\n",
        "            self.state[self.ptr:self.ptr+batch_size] = state\n",
        "            self.action[self.ptr:self.ptr+batch_size] = action\n",
        "            self.reward[self.ptr:self.ptr+batch_size] = reward.reshape(-1, 1)\n",
        "            self.next_state[self.ptr:self.ptr+batch_size] = next_state\n",
        "            self.done[self.ptr:self.ptr+batch_size] = done.reshape(-1, 1)\n",
        "            self.ptr = (self.ptr + batch_size) % self.buffer_size\n",
        "        else:\n",
        "            # Handle wrap around\n",
        "            overflow = (self.ptr + batch_size) - self.buffer_size\n",
        "            split = batch_size - overflow\n",
        "\n",
        "            # First part\n",
        "            self.state[self.ptr:] = state[:split]\n",
        "            self.action[self.ptr:] = action[:split]\n",
        "            self.reward[self.ptr:] = reward[:split].reshape(-1, 1)\n",
        "            self.next_state[self.ptr:] = next_state[:split]\n",
        "            self.done[self.ptr:] = done[:split].reshape(-1, 1)\n",
        "\n",
        "            # Second part (overflow)\n",
        "            self.state[:overflow] = state[split:]\n",
        "            self.action[:overflow] = action[split:]\n",
        "            self.reward[:overflow] = reward[split:].reshape(-1, 1)\n",
        "            self.next_state[:overflow] = next_state[split:]\n",
        "            self.done[:overflow] = done[split:].reshape(-1, 1)\n",
        "            self.ptr = overflow\n",
        "\n",
        "        self.size = min(self.size + batch_size, self.buffer_size)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, self.size, size=batch_size)\n",
        "        return (\n",
        "            self.state[ind],\n",
        "            self.action[ind],\n",
        "            self.reward[ind],\n",
        "            self.next_state[ind],\n",
        "            self.done[ind]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdyjBjBOIYv9"
      },
      "source": [
        "## Network Architectures\n",
        "\n",
        "We define the Actor and Critic networks. The Actor outputs the mean and log standard deviation of the action distribution. The Critic estimates the Q-value for a given state-action pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "N4LvxZJaIYv-"
      },
      "outputs": [],
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.mu = nn.Linear(hidden_dim, action_dim)\n",
        "        # Fix: log_std is now a layer that depends on the state\n",
        "        self.log_std_linear = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        mu = self.mu(x)\n",
        "        log_std = self.log_std_linear(x)\n",
        "        # Constrain log_std to prevent gradients from exploding\n",
        "        log_std = torch.clamp(log_std, -20, 2)\n",
        "        return mu, log_std\n",
        "\n",
        "    def sample(self, state):\n",
        "        mu, log_std = self.forward(state)\n",
        "        std = log_std.exp()\n",
        "        dist = Normal(mu, std)\n",
        "        x_t = dist.rsample()  # Reparameterization trick\n",
        "        action = torch.tanh(x_t)\n",
        "\n",
        "        # Correct log_prob for tanh squashing\n",
        "        log_prob = dist.log_prob(x_t)\n",
        "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
        "        log_prob = log_prob.sum(-1, keepdim=True)\n",
        "        return action, log_prob\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        # Critic takes state and action as input\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.q_value = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.q_value(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnImJs5sIYv-"
      },
      "source": [
        "## SAC Agent\n",
        "\n",
        "The SAC agent orchestrates the interaction with the environment and the training process. It maintains the actor, two critics (for double Q-learning), and their target networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Zj9UhMYTIYv-"
      },
      "outputs": [],
      "source": [
        "class SACAgent:\n",
        "    def __init__(self, state_dim, action_dim, action_scale=1.0, device=\"cpu\", learning_rate=3e-4):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.action_scale = action_scale\n",
        "        self.device = device\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "        self.alpha = 0.2\n",
        "        self.batch_size = 256\n",
        "        self.buffer_size = int(1e6)\n",
        "        # Automatic entropy tuning target: -dim(A)\n",
        "        self.target_entropy = -float(action_dim)\n",
        "\n",
        "        # Networks\n",
        "        self.actor = ActorNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.critic1 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.critic2 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.target_critic1 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
        "        self.target_critic2 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
        "\n",
        "        # Copy weights to target networks\n",
        "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
        "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
        "\n",
        "        # Optimizers\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.learning_rate)\n",
        "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.learning_rate)\n",
        "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Replay buffer (Now using optimized Numpy buffer with batch support)\n",
        "        self.replay_buffer = ReplayBuffer(state_dim, action_dim, self.buffer_size)\n",
        "\n",
        "        # Log alpha for entropy adjustment\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
        "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.learning_rate)\n",
        "\n",
        "    def select_action(self, state, deterministic=False):\n",
        "        with torch.no_grad():\n",
        "            if state.ndim == 1:\n",
        "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            else:\n",
        "                state = torch.FloatTensor(state).to(self.device)\n",
        "\n",
        "            if deterministic:\n",
        "                mu, _ = self.actor(state)\n",
        "                action = torch.tanh(mu)\n",
        "                return action.cpu().numpy() # Return batch if input was batch\n",
        "\n",
        "            action, _ = self.actor.sample(state)\n",
        "            return action.cpu().numpy()\n",
        "\n",
        "    def update(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample from replay buffer\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.FloatTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        # Update critic\n",
        "        with torch.no_grad():\n",
        "            next_actions, next_log_pi = self.actor.sample(next_states)\n",
        "            next_q1 = self.target_critic1(next_states, next_actions)\n",
        "            next_q2 = self.target_critic2(next_states, next_actions)\n",
        "            next_q = torch.min(next_q1, next_q2) - self.alpha * next_log_pi\n",
        "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
        "\n",
        "        current_q1 = self.critic1(states, actions)\n",
        "        current_q2 = self.critic2(states, actions)\n",
        "\n",
        "        critic1_loss = F.mse_loss(current_q1, target_q)\n",
        "        critic2_loss = F.mse_loss(current_q2, target_q)\n",
        "\n",
        "        self.critic1_optimizer.zero_grad()\n",
        "        critic1_loss.backward()\n",
        "        self.critic1_optimizer.step()\n",
        "\n",
        "        self.critic2_optimizer.zero_grad()\n",
        "        critic2_loss.backward()\n",
        "        self.critic2_optimizer.step()\n",
        "\n",
        "        # Update actor\n",
        "        actions_pred, log_pi = self.actor.sample(states)\n",
        "        q1 = self.critic1(states, actions_pred)\n",
        "        q2 = self.critic2(states, actions_pred)\n",
        "        q = torch.min(q1, q2)\n",
        "\n",
        "        actor_loss = (self.alpha * log_pi - q).mean()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        # Failsafe for target_entropy\n",
        "        if not hasattr(self, 'target_entropy'):\n",
        "            self.target_entropy = -float(self.action_dim)\n",
        "\n",
        "        # Update alpha\n",
        "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
        "\n",
        "        self.alpha_optimizer.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.alpha_optimizer.step()\n",
        "        self.alpha = self.log_alpha.exp()\n",
        "\n",
        "        # Update target networks\n",
        "        for param, target_param in zip(self.critic1.parameters(), self.target_critic1.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "        for param, target_param in zip(self.critic2.parameters(), self.target_critic2.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4CWIs7CIYv-"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "We train the agent in the environment. We'll also log rewards and save checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kXFMT3bSIYv-"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import logging\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# Define a custom handler to work with tqdm\n",
        "class TqdmLoggingHandler(logging.Handler):\n",
        "    def __init__(self, level=logging.NOTSET):\n",
        "        super().__init__(level)\n",
        "\n",
        "    def emit(self, record):\n",
        "        try:\n",
        "            msg = self.format(record)\n",
        "            tqdm.write(msg)\n",
        "            self.flush()\n",
        "        except Exception:\n",
        "            self.handleError(record)\n",
        "\n",
        "def setup_logger(log_dir=\"logs\"):\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "    logger = logging.getLogger(\"BipedalWalker\")\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "    # Clear existing handlers to avoid duplicate logs\n",
        "    if logger.hasHandlers():\n",
        "        logger.handlers.clear()\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    log_file = os.path.join(log_dir, f\"training_{timestamp}.log\")\n",
        "\n",
        "    # File handler\n",
        "    fh = logging.FileHandler(log_file)\n",
        "    fh.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    fh.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "\n",
        "    # Console handler using TqdmLoggingHandler\n",
        "    ch = TqdmLoggingHandler()\n",
        "    ch.setLevel(logging.INFO)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(ch)\n",
        "\n",
        "    return logger\n",
        "\n",
        "class EfficientWalkerWrapper(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, truncated, info = self.env.step(action)\n",
        "\n",
        "        # state[2] is horizontal velocity in BipedalWalker-v3\n",
        "        forward_vel = state[2]\n",
        "\n",
        "        # Penalize excessive torque to prevent the \"dragging\" behavior\n",
        "        # and encourage a natural, energy-efficient gait.\n",
        "        energy_penalty = -0.001 * np.sum(np.square(action))\n",
        "\n",
        "        # Add a small stability bonus for keeping the body level\n",
        "        # state[0] is hull angle\n",
        "        stability_reward = -0.1 * abs(state[0])\n",
        "\n",
        "        reward += (forward_vel * 0.5) + energy_penalty + stability_reward\n",
        "\n",
        "        return state, reward, done, truncated, info\n",
        "\n",
        "def save_checkpoint(agent, episode, learning_rate, log_dir=\"logs\", filename=None):\n",
        "    if filename is None:\n",
        "        filename = f\"checkpoint_ep{episode}.pth\"\n",
        "\n",
        "    path = os.path.join(log_dir, filename)\n",
        "    torch.save({\n",
        "        'actor_state_dict': agent.actor.state_dict(),\n",
        "        'critic1_state_dict': agent.critic1.state_dict(),\n",
        "        'critic2_state_dict': agent.critic2.state_dict(),\n",
        "        'episode': episode,\n",
        "        'learning_rate': learning_rate\n",
        "    }, path)\n",
        "    return path\n",
        "\n",
        "def train_agent(env_name=\"BipedalWalker-v3\", max_episodes=1000, max_steps=1000, device=\"cpu\", render_freq=50, learning_rate=3e-4, updates_per_step=1, start_steps=20000, num_envs=1, save_interval=10, log_dir=None):\n",
        "    # Determine log directory\n",
        "    if log_dir is None:\n",
        "        log_dir = os.path.join(\"logs\", datetime.now().strftime(\"run_%Y%m%d_%H%M%S\"))\n",
        "\n",
        "    # Setup logger\n",
        "    logger = setup_logger(log_dir=log_dir)\n",
        "    logger.info(f\"Starting training with device: {device}, LR: {learning_rate}, Num Envs: {num_envs}\")\n",
        "    logger.info(f\"Logs and checkpoints will be saved to: {log_dir}\")\n",
        "\n",
        "    # Determine render mode\n",
        "    # Always use rgb_array to allow conditional rendering without re-init\n",
        "    render_mode = \"rgb_array\"\n",
        "\n",
        "    # Track per-environment progress\n",
        "    env_step_counts = np.zeros(num_envs, dtype=int)\n",
        "    env_episode_counts = np.zeros(num_envs, dtype=int)\n",
        "\n",
        "    # Create environment\n",
        "    if num_envs > 1:\n",
        "        # Use EfficientWalkerWrapper\n",
        "        vec_mode = \"async\" # Async is usually faster\n",
        "        env = gym.make_vec(env_name, num_envs=num_envs, vectorization_mode=vec_mode, wrappers=[EfficientWalkerWrapper], render_mode=render_mode)\n",
        "        logger.info(f\"Using {num_envs} vectorized environments ({vec_mode}) with EfficientWalkerWrapper\")\n",
        "    else:\n",
        "        env = gym.make(env_name, render_mode=render_mode)\n",
        "        env = EfficientWalkerWrapper(env)\n",
        "        logger.info(f\"Using EfficientWalkerWrapper\")\n",
        "\n",
        "    if num_envs > 1:\n",
        "        state_dim = env.single_observation_space.shape[0]\n",
        "        action_dim = env.single_action_space.shape[0]\n",
        "        action_scale = float(env.single_action_space.high[0])\n",
        "    else:\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0]\n",
        "        action_scale = float(env.action_space.high[0])\n",
        "\n",
        "    # Initialize agent\n",
        "    logger.info(f\"Initializing SAC Agent on device: {device}\")\n",
        "    agent = SACAgent(state_dim, action_dim, action_scale, device=device, learning_rate=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    total_steps = 0\n",
        "    episode_rewards = []\n",
        "\n",
        "    # Create logs directory if it doesn't exist\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(range(max_episodes), desc=f\"Training Progress\", unit=\"ep\")\n",
        "\n",
        "    current_episode = 0\n",
        "\n",
        "    # Reset env\n",
        "    state, _ = env.reset()\n",
        "\n",
        "    try:\n",
        "        while current_episode < max_episodes:\n",
        "            episode_reward = 0\n",
        "            if num_envs > 1:\n",
        "                current_rewards = np.zeros(num_envs)\n",
        "\n",
        "            # Determine if we should render this episode\n",
        "            should_render = (render_freq > 0) and (current_episode % render_freq == 0)\n",
        "\n",
        "            for step in range(max_steps):\n",
        "                # Select action\n",
        "                if total_steps < start_steps:\n",
        "                    if num_envs > 1:\n",
        "                        action = np.array([env.single_action_space.sample() for _ in range(num_envs)])\n",
        "                    else:\n",
        "                        action = env.action_space.sample()\n",
        "                else:\n",
        "                    if num_envs > 1:\n",
        "                        action = agent.select_action(state, deterministic=False)\n",
        "                    else:\n",
        "                        action = agent.select_action(state)\n",
        "\n",
        "                # Take step\n",
        "                next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "                # Rendering logic\n",
        "                if should_render:\n",
        "                    try:\n",
        "                        frames = env.render()\n",
        "                        # Display frames logic\n",
        "                        if isinstance(frames, (tuple, list)) and len(frames) == num_envs:\n",
        "                             # Render only the first environment\n",
        "                             frame = frames[0]\n",
        "                             if isinstance(frame, np.ndarray):\n",
        "                                bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                                cv2.putText(bgr_frame, f\"Ep: {current_episode}\", (10, 30),\n",
        "                                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "                                cv2.imshow(f\"Training Preview\", bgr_frame)\n",
        "                                cv2.waitKey(1)\n",
        "                        elif isinstance(frames, np.ndarray):\n",
        "                             bgr_frame = cv2.cvtColor(frames, cv2.COLOR_RGB2BGR)\n",
        "                             cv2.putText(bgr_frame, f\"Ep: {current_episode}\", (10, 30),\n",
        "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
        "                             cv2.imshow(\"Training Preview\", bgr_frame)\n",
        "                             cv2.waitKey(1)\n",
        "                    except Exception as e:\n",
        "                        # Sometimes rendering fails if window closed\n",
        "                        pass\n",
        "\n",
        "                # Handle done/truncated\n",
        "                if num_envs > 1:\n",
        "                    done_flag = done | truncated\n",
        "                    current_rewards += reward\n",
        "\n",
        "                    for i in range(num_envs):\n",
        "                        if done_flag[i]:\n",
        "                            episode_rewards.append(current_rewards[i])\n",
        "                            current_rewards[i] = 0\n",
        "                            current_episode += 1\n",
        "                            pbar.update(1)\n",
        "\n",
        "                            # Update render status\n",
        "                            should_render = (render_freq > 0) and (current_episode % render_freq == 0)\n",
        "\n",
        "                            # Checkpoint logic\n",
        "                            if current_episode % save_interval == 0:\n",
        "                                path = save_checkpoint(agent, current_episode, learning_rate, log_dir=log_dir)\n",
        "                                logger.info(f\"Checkpoint saved at episode {current_episode}: {path}\")\n",
        "\n",
        "                            if current_episode >= max_episodes:\n",
        "                                break\n",
        "\n",
        "                    # Buffer addition for vec env\n",
        "                    real_next_states = next_state.copy()\n",
        "                    if \"_final_observation\" in info:\n",
        "                        mask = info[\"_final_observation\"]\n",
        "                        for i, is_final in enumerate(mask):\n",
        "                            if is_final and \"final_observation\" in info:\n",
        "                                real_next_states[i] = info[\"final_observation\"][i]\n",
        "\n",
        "                    agent.replay_buffer.add(state, action, reward, real_next_states, done_flag)\n",
        "\n",
        "                else:\n",
        "                    done_flag = done or truncated\n",
        "                    agent.replay_buffer.add(state, action, reward, next_state, done_flag)\n",
        "                    episode_reward += reward\n",
        "\n",
        "                    if done_flag:\n",
        "                        episode_rewards.append(episode_reward)\n",
        "                        current_episode += 1\n",
        "                        pbar.update(1)\n",
        "\n",
        "                        should_render = (render_freq > 0) and (current_episode % render_freq == 0)\n",
        "\n",
        "                        # Checkpoint logic\n",
        "                        if current_episode % save_interval == 0:\n",
        "                             path = save_checkpoint(agent, current_episode, learning_rate, log_dir=log_dir)\n",
        "                             logger.info(f\"Checkpoint saved at episode {current_episode}: {path}\")\n",
        "\n",
        "                        state, _ = env.reset()\n",
        "                        break\n",
        "\n",
        "                state = next_state\n",
        "                total_steps += num_envs\n",
        "\n",
        "                # Update agent\n",
        "                if len(agent.replay_buffer) > agent.batch_size and total_steps >= start_steps:\n",
        "                    for _ in range(updates_per_step * num_envs):\n",
        "                        agent.update()\n",
        "\n",
        "            # Logging progress occasionally\n",
        "            if len(episode_rewards) > 0 and current_episode % 10 == 0:\n",
        "                avg_reward = np.mean(episode_rewards[-10:])\n",
        "                pbar.set_postfix({\n",
        "                    'Last Reward': f'{episode_rewards[-1]:.2f}',\n",
        "                    'Avg Reward (10)': f'{avg_reward:.2f}',\n",
        "                    'Steps': total_steps\n",
        "                })\n",
        "                logger.info(f\"Episode {current_episode}: Avg Reward (10) = {avg_reward:.2f}, Total Steps = {total_steps}\")\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        logger.warning(\"Training interrupted by user. Saving emergency checkpoint...\")\n",
        "        path = save_checkpoint(agent, current_episode, learning_rate, log_dir=log_dir, filename=f\"emergency_checkpoint_ep{current_episode}.pth\")\n",
        "        logger.info(f\"Emergency checkpoint saved: {path}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error occurred: {e}\")\n",
        "        raise e\n",
        "    finally:\n",
        "        try:\n",
        "            cv2.destroyAllWindows()\n",
        "            cv2.waitKey(1)\n",
        "        except:\n",
        "            pass\n",
        "        env.close()\n",
        "        logger.info(\"Training finished/stopped.\")\n",
        "\n",
        "    return episode_rewards, agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6y6k0sbIYv-"
      },
      "source": [
        "## Execution\n",
        "\n",
        "Run the training loop and plot the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY68Oa2CIYv-",
        "outputId": "372da6d6-e7eb-4476-8f4f-99c196b2e2ee"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:BipedalWalker:Starting training with device: cpu, LR: 0.0003, Num Envs: 32\n",
            "INFO:BipedalWalker:Logs and checkpoints will be saved to: logs/run_20251226_075338\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting BipedalWalker training on cpu...\n",
            "2025-12-26 07:53:38,382 - INFO - Starting training with device: cpu, LR: 0.0003, Num Envs: 32\n",
            "2025-12-26 07:53:38,384 - INFO - Logs and checkpoints will be saved to: logs/run_20251226_075338\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:BipedalWalker:Using 32 vectorized environments (async) with EfficientWalkerWrapper\n",
            "INFO:BipedalWalker:Initializing SAC Agent on device: cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-26 07:53:39,382 - INFO - Using 32 vectorized environments (async) with EfficientWalkerWrapper\n",
            "2025-12-26 07:53:39,384 - INFO - Initializing SAC Agent on device: cpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   1%|▏         | 10/700 [00:04<02:24,  4.79ep/s]INFO:BipedalWalker:Checkpoint saved at episode 10: logs/run_20251226_075338/checkpoint_ep10.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-26 07:53:49,529 - INFO - Checkpoint saved at episode 10: logs/run_20251226_075338/checkpoint_ep10.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   3%|▎         | 20/700 [00:07<04:51,  2.33ep/s]INFO:BipedalWalker:Checkpoint saved at episode 20: logs/run_20251226_075338/checkpoint_ep20.pth\n",
            "Training Progress:   3%|▎         | 21/700 [00:07<02:07,  5.35ep/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-26 07:53:52,569 - INFO - Checkpoint saved at episode 20: logs/run_20251226_075338/checkpoint_ep20.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   4%|▍         | 30/700 [00:09<03:16,  3.41ep/s]INFO:BipedalWalker:Checkpoint saved at episode 30: logs/run_20251226_075338/checkpoint_ep30.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-26 07:53:55,339 - INFO - Checkpoint saved at episode 30: logs/run_20251226_075338/checkpoint_ep30.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   6%|▌         | 40/700 [00:13<03:54,  2.81ep/s]INFO:BipedalWalker:Checkpoint saved at episode 40: logs/run_20251226_075338/checkpoint_ep40.pth\n",
            "Training Progress:   6%|▌         | 41/700 [00:13<03:11,  3.43ep/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-26 07:53:58,881 - INFO - Checkpoint saved at episode 40: logs/run_20251226_075338/checkpoint_ep40.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:   7%|▋         | 50/700 [00:23<13:34,  1.25s/ep]INFO:BipedalWalker:Checkpoint saved at episode 50: logs/run_20251226_075338/checkpoint_ep50.pth\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-12-26 07:54:08,487 - INFO - Checkpoint saved at episode 50: logs/run_20251226_075338/checkpoint_ep50.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   9%|▊         | 60/700 [09:17<7:23:21, 41.57s/ep]INFO:BipedalWalker:Checkpoint saved at episode 60: logs/run_20251226_075338/checkpoint_ep60.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-12-26 08:03:02,785 - INFO - Checkpoint saved at episode 60: logs/run_20251226_075338/checkpoint_ep60.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Progress:   9%|▉         | 64/700 [10:12<3:20:19, 18.90s/ep]"
          ]
        }
      ],
      "source": [
        "# Start training\n",
        "print(f\"Starting BipedalWalker training on {device}...\")\n",
        "\n",
        "# Configurable Hyperparameters\n",
        "MAX_EPISODES = 700\n",
        "UPDATES_PER_STEP = 1  # Reduced to 1 for speed\n",
        "NUM_ENVS = 32 # Number of parallel environments (vectorized)\n",
        "START_STEPS = 20000 # Increased exploration\n",
        "RENDER_FREQ = 0 # Set to 0 to disable rendering for 10x speedup\n",
        "\n",
        "rewards, trained_agent = train_agent(\n",
        "    max_episodes=MAX_EPISODES,\n",
        "    device=device,\n",
        "    updates_per_step=UPDATES_PER_STEP,\n",
        "    start_steps=START_STEPS,\n",
        "    num_envs=NUM_ENVS,\n",
        "    render_freq=RENDER_FREQ\n",
        ")\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(rewards)\n",
        "plt.title(\"Training Rewards\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYnxq-XqIYv-"
      },
      "source": [
        "## Video Recording and Evaluation\n",
        "\n",
        "After training, we can record a video of the agent's performance to visually verify its walking ability. This is required for the application."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfLUrhhnIYv-"
      },
      "outputs": [],
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "from IPython.display import Video\n",
        "import glob\n",
        "import os\n",
        "\n",
        "def load_checkpoint(checkpoint_path, state_dim, action_dim, device=\"cpu\", learning_rate=3e-4):\n",
        "    \"\"\"Load a trained agent from a checkpoint.\"\"\"\n",
        "    agent = SACAgent(state_dim, action_dim, device=device, learning_rate=learning_rate)\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    # Load with strict=False to handle architecture changes (old log_std param vs new log_std_linear layer)\n",
        "    agent.actor.load_state_dict(checkpoint['actor_state_dict'], strict=False)\n",
        "    agent.critic1.load_state_dict(checkpoint['critic1_state_dict'], strict=False)\n",
        "    agent.critic2.load_state_dict(checkpoint['critic2_state_dict'], strict=False)\n",
        "\n",
        "    print(f\"Loaded checkpoint from: {checkpoint_path}\")\n",
        "    return agent\n",
        "\n",
        "def record_video(agent, env_name=\"BipedalWalker-v3\", filename=\"bipedal_walker\", device=\"cpu\"):\n",
        "    # Create environment with render mode\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "\n",
        "    # Wrap environment to record video\n",
        "    # We force record the first episode\n",
        "    video_folder = \"videos\"\n",
        "    os.makedirs(video_folder, exist_ok=True)\n",
        "    env = RecordVideo(env, video_folder=video_folder, name_prefix=filename, episode_trigger=lambda x: True)\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    truncated = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not (done or truncated):\n",
        "        # Use deterministic policy for evaluation\n",
        "        action = agent.select_action(state, deterministic=True)\n",
        "\n",
        "        # Ensure action is 1D if it comes back as 2D batch (handle legacy/stale agent instances)\n",
        "        if isinstance(action, np.ndarray) and action.ndim > 1:\n",
        "             action = action.flatten()\n",
        "\n",
        "        next_state, reward, done, truncated, _ = env.step(action)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    env.close()\n",
        "    print(f\"Evaluation Run - Total Reward: {total_reward:.2f}\")\n",
        "\n",
        "    # Find the video file\n",
        "    mp4_files = glob.glob(f\"{video_folder}/{filename}-episode-0.mp4\")\n",
        "    if mp4_files:\n",
        "        print(f\"Video saved to {mp4_files[0]}\")\n",
        "        return mp4_files[0]\n",
        "    return None\n",
        "\n",
        "# Record and display video from latest checkpoint\n",
        "# Find the latest checkpoint in the logs directory\n",
        "log_dirs = glob.glob(\"logs/run_*\")\n",
        "if log_dirs:\n",
        "    latest_log_dir = max(log_dirs, key=os.path.getctime)\n",
        "    checkpoints = glob.glob(f\"{latest_log_dir}/checkpoint_ep*.pth\")\n",
        "\n",
        "    if checkpoints:\n",
        "        # Select by highest episode number rather than filesystem time\n",
        "        def _ep_num(path):\n",
        "            name = os.path.basename(path)\n",
        "            try:\n",
        "                return int(name.split(\"checkpoint_ep\")[1].split(\".pth\")[0])\n",
        "            except Exception:\n",
        "                return -1\n",
        "\n",
        "        latest_checkpoint = max(checkpoints, key=_ep_num)\n",
        "        print(f\"Loading agent from highest-episode checkpoint: {latest_checkpoint}\")\n",
        "\n",
        "        # Create a fresh agent and load the checkpoint\n",
        "        env_temp = gym.make(\"BipedalWalker-v3\")\n",
        "        state_dim = env_temp.observation_space.shape[0]\n",
        "        action_dim = env_temp.action_space.shape[0]\n",
        "        env_temp.close()\n",
        "\n",
        "        checkpoint_agent = load_checkpoint(latest_checkpoint, state_dim, action_dim, device=device)\n",
        "\n",
        "        # Use log directory name as video filename\n",
        "        log_dir_name = os.path.basename(latest_log_dir)\n",
        "\n",
        "        # Record video with the loaded agent\n",
        "        video_path = record_video(checkpoint_agent, filename=log_dir_name, device=device)\n",
        "        if video_path:\n",
        "            display(Video(video_path, embed=True, html_attributes=\"controls autoplay loop\"))\n",
        "    else:\n",
        "        print(\"No checkpoints found in logs directory\")\n",
        "else:\n",
        "    print(\"No log directories found. Train the agent first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3Ic8vKJIYv-"
      },
      "outputs": [],
      "source": [
        "def run_ablation_study(device=\"cpu\"):\n",
        "    # Compare different learning rates\n",
        "    learning_rates = [1e-3, 3e-4, 1e-4]\n",
        "    results = {}\n",
        "\n",
        "    print(\"Starting Ablation Study on Learning Rates...\")\n",
        "\n",
        "    # Create a timestamped directory for the ablation study\n",
        "    study_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    study_dir = os.path.join(\"logs\", f\"ablation_{study_timestamp}\")\n",
        "    os.makedirs(study_dir, exist_ok=True)\n",
        "    print(f\"Ablation results will be saved to: {study_dir}\")\n",
        "\n",
        "    for lr in learning_rates:\n",
        "        print(f\"\\nTesting Learning Rate: {lr}\")\n",
        "        # Run training for fewer episodes for ablation study to save time, or full duration if desired\n",
        "        # Using 500 episodes for ablation study demonstration\n",
        "        ablation_episodes = 500\n",
        "\n",
        "        # Use a separate subfolder for each learning rate to avoid file conflicts\n",
        "        # and to keep logs organized\n",
        "        run_log_dir = os.path.join(study_dir, f\"lr_{lr}\")\n",
        "\n",
        "        # render_freq=0 disables rendering for speed\n",
        "        rewards, _ = train_agent(max_episodes=ablation_episodes, device=device, learning_rate=lr, log_dir=run_log_dir, render_freq=0)\n",
        "        results[f\"lr_{lr}\"] = rewards\n",
        "\n",
        "        # Save intermediate result in the study directory\n",
        "        np.save(os.path.join(study_dir, f\"ablation_rewards_lr_{lr}.npy\"), rewards)\n",
        "\n",
        "    print(\"Ablation study completed. Plotting results...\")\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for name, rewards in results.items():\n",
        "        # Smooth rewards for better visualization\n",
        "        window = 10\n",
        "        smoothed_rewards = [np.mean(rewards[max(0, i-window):i+1]) for i in range(len(rewards))]\n",
        "        plt.plot(smoothed_rewards, label=name)\n",
        "\n",
        "    plt.title(\"Ablation Study: Learning Rates (Smoothed)\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Run the ablation study\n",
        "run_ablation_study(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "shutil.copytree(\n",
        "    '/content/',\n",
        "    '/content/drive/MyDrive/log',\n",
        "    dirs_exist_ok=True\n",
        ")"
      ],
      "metadata": {
        "id": "OCuUNBghohJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pTqjn0SIYv-"
      },
      "source": [
        "# AISF Application Writeup\n",
        "\n",
        "## Applicant Information\n",
        "- **Name**: [Your Name]\n",
        "- **Email**: [Your Email]\n",
        "\n",
        "## Prior Experience\n",
        "*Briefly describe any prior experience you have in RL, Deep Learning, and coding.*\n",
        "\n",
        "## Time Breakdown\n",
        "*Total hours spent: X hours*\n",
        "- Research/Reading: X hours\n",
        "- Coding: X hours\n",
        "- Training/Tuning: X hours\n",
        "- Writing: X hours\n",
        "\n",
        "## Compute Resources\n",
        "*Describe the hardware used.*\n",
        "- CPU: [Number of threads, Model]\n",
        "- GPU: [Model] (if applicable)\n",
        "\n",
        "## Techniques Used\n",
        "*List the techniques used (e.g., SAC, Double Q-Learning, Soft Updates) and justify why they are suitable for this environment.*\n",
        "\n",
        "## Ablation Studies\n",
        "*Summarize the results of your ablation studies here. What hyperparameters did you tweak? What was the impact?*\n",
        "\n",
        "## Discussion of Issues\n",
        "*Discuss challenges encountered, ideas that didn't work, and how you pivoted.*\n",
        "\n",
        "## Conclusion\n",
        "*Summarize what worked well and what could be improved with more time.*\n",
        "\n",
        "## Citations\n",
        "- [Haarnoja, T., et al. \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\" arXiv preprint arXiv:1801.01290 (2018).](https://arxiv.org/abs/1801.01290)\n",
        "- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsYtwhhgIYv-"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rl-gym",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}