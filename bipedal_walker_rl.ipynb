{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC) for BipedalWalker-v3\n",
    "\n",
    "This notebook implements a Soft Actor-Critic (SAC) agent to solve the BipedalWalker-v3 environment from Gymnasium.\n",
    "SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swig\n",
    "# !pip install gymnasium[box2d]\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import cv2  # Added for parallel window rendering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Change to standard tqdm to avoid notebook widget errors\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# For local training on Mac, \n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        torch.cuda.set_device(0)\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"CUDA GPU: {gpu_name} | Memory: {props.total_memory/1024**3:.1f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU info not available: {e}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer stores experience tuples (state, action, reward, next_state, done) to be sampled during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, buffer_size=int(1e6)):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.state = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
    "        self.action = np.zeros((buffer_size, action_dim), dtype=np.float32)\n",
    "        self.reward = np.zeros((buffer_size, 1), dtype=np.float32)\n",
    "        self.next_state = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
    "        self.done = np.zeros((buffer_size, 1), dtype=np.float32)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Check if input is batch or single\n",
    "        if state.ndim == 1:\n",
    "            state = state[None, :]\n",
    "            action = action[None, :]\n",
    "            reward = np.array(reward)[None]\n",
    "            next_state = next_state[None, :]\n",
    "            done = np.array(done)[None]\n",
    "        \n",
    "        batch_size = len(state)\n",
    "        \n",
    "        if self.ptr + batch_size <= self.buffer_size:\n",
    "            self.state[self.ptr:self.ptr+batch_size] = state\n",
    "            self.action[self.ptr:self.ptr+batch_size] = action\n",
    "            self.reward[self.ptr:self.ptr+batch_size] = reward.reshape(-1, 1)\n",
    "            self.next_state[self.ptr:self.ptr+batch_size] = next_state\n",
    "            self.done[self.ptr:self.ptr+batch_size] = done.reshape(-1, 1)\n",
    "            self.ptr = (self.ptr + batch_size) % self.buffer_size\n",
    "        else:\n",
    "            # Handle wrap around\n",
    "            overflow = (self.ptr + batch_size) - self.buffer_size\n",
    "            split = batch_size - overflow\n",
    "            \n",
    "            # First part\n",
    "            self.state[self.ptr:] = state[:split]\n",
    "            self.action[self.ptr:] = action[:split]\n",
    "            self.reward[self.ptr:] = reward[:split].reshape(-1, 1)\n",
    "            self.next_state[self.ptr:] = next_state[:split]\n",
    "            self.done[self.ptr:] = done[:split].reshape(-1, 1)\n",
    "            \n",
    "            # Second part (overflow)\n",
    "            self.state[:overflow] = state[split:]\n",
    "            self.action[:overflow] = action[split:]\n",
    "            self.reward[:overflow] = reward[split:].reshape(-1, 1)\n",
    "            self.next_state[:overflow] = next_state[split:]\n",
    "            self.done[:overflow] = done[split:].reshape(-1, 1)\n",
    "            self.ptr = overflow\n",
    "            \n",
    "        self.size = min(self.size + batch_size, self.buffer_size)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            self.state[ind],\n",
    "            self.action[ind],\n",
    "            self.reward[ind],\n",
    "            self.next_state[ind],\n",
    "            self.done[ind]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architectures\n",
    "\n",
    "We define the Actor and Critic networks. The Actor outputs the mean and log standard deviation of the action distribution. The Critic estimates the Q-value for a given state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, action_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim) * 0.1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "        return mu, self.log_std\n",
    "    \n",
    "    def sample(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        x_t = dist.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        \n",
    "        # Log prob calculation\n",
    "        log_prob = dist.log_prob(x_t)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        # Critic takes state and action as input\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q_value = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.q_value(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC Agent\n",
    "\n",
    "The SAC agent orchestrates the interaction with the environment and the training process. It maintains the actor, two critics (for double Q-learning), and their target networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(self, state_dim, action_dim, action_scale=1.0, device=\"cpu\", learning_rate=3e-4):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_scale = action_scale\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        self.alpha = 0.2\n",
    "        self.batch_size = 256\n",
    "        self.buffer_size = int(1e6)\n",
    "        # Automatic entropy tuning target: -dim(A)\n",
    "        self.target_entropy = -float(action_dim)\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = ActorNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.critic1 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.critic2 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_critic1 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_critic2 = CriticNetwork(state_dim, action_dim).to(self.device)\n",
    "        \n",
    "        # Copy weights to target networks\n",
    "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.learning_rate)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Replay buffer (Now using optimized Numpy buffer with batch support)\n",
    "        self.replay_buffer = ReplayBuffer(state_dim, action_dim, self.buffer_size)\n",
    "        \n",
    "        # Log alpha for entropy adjustment\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.learning_rate)\n",
    "        \n",
    "    def select_action(self, state, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            if state.ndim == 1:\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            else:\n",
    "                state = torch.FloatTensor(state).to(self.device)\n",
    "                \n",
    "            if deterministic:\n",
    "                mu, _ = self.actor(state)\n",
    "                action = torch.tanh(mu)\n",
    "                return action.cpu().numpy() # Return batch if input was batch\n",
    "            \n",
    "            action, _ = self.actor.sample(state)\n",
    "            return action.cpu().numpy()\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "            \n",
    "        # Sample from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_pi = self.actor.sample(next_states)\n",
    "            next_q1 = self.target_critic1(next_states, next_actions)\n",
    "            next_q2 = self.target_critic2(next_states, next_actions)\n",
    "            next_q = torch.min(next_q1, next_q2) - self.alpha * next_log_pi\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        current_q1 = self.critic1(states, actions)\n",
    "        current_q2 = self.critic2(states, actions)\n",
    "        \n",
    "        critic1_loss = F.mse_loss(current_q1, target_q)\n",
    "        critic2_loss = F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        self.critic1_optimizer.step()\n",
    "        \n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        self.critic2_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        actions_pred, log_pi = self.actor.sample(states)\n",
    "        q1 = self.critic1(states, actions_pred)\n",
    "        q2 = self.critic2(states, actions_pred)\n",
    "        q = torch.min(q1, q2)\n",
    "        \n",
    "        actor_loss = (self.alpha * log_pi - q).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Failsafe for target_entropy\n",
    "        if not hasattr(self, 'target_entropy'):\n",
    "            self.target_entropy = -float(self.action_dim)\n",
    "            \n",
    "        # Update alpha\n",
    "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "        \n",
    "        # Update target networks\n",
    "        for param, target_param in zip(self.critic1.parameters(), self.target_critic1.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            \n",
    "        for param, target_param in zip(self.critic2.parameters(), self.target_critic2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "We train the agent in the environment. We'll also log rewards and save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Define a custom handler to work with tqdm\n",
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def __init__(self, level=logging.NOTSET):\n",
    "        super().__init__(level)\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "def setup_logger(log_dir=\"logs\"):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    logger = logging.getLogger(\"BipedalWalker\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Clear existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "        \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = os.path.join(log_dir, f\"training_{timestamp}.log\")\n",
    "    \n",
    "    # File handler\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    \n",
    "    # Console handler using TqdmLoggingHandler\n",
    "    ch = TqdmLoggingHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class AlternatingLegsRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, scale=1.0):\n",
    "        super().__init__(env)\n",
    "        self.scale = scale\n",
    "        \n",
    "    def step(self, action):\n",
    "        state, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # BipedalWalker-v3 Observation Space indices:\n",
    "        # 4: hip_joint_1_angle\n",
    "        # 5: hip_joint_1_speed\n",
    "        # 9: hip_joint_2_angle\n",
    "        # 10: hip_joint_2_speed\n",
    "        \n",
    "        hip1_speed = state[5]\n",
    "        hip2_speed = state[10]\n",
    "        \n",
    "        # Reward for moving hips in opposite directions\n",
    "        alternating_reward = - (hip1_speed * hip2_speed)\n",
    "        \n",
    "        # Add to total reward\n",
    "        reward += alternating_reward * self.scale\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "def save_checkpoint(agent, episode, learning_rate, log_dir=\"logs\", filename=None):\n",
    "    if filename is None:\n",
    "        filename = f\"checkpoint_ep{episode}.pth\"\n",
    "    \n",
    "    path = os.path.join(log_dir, filename)\n",
    "    torch.save({\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic1_state_dict': agent.critic1.state_dict(),\n",
    "        'critic2_state_dict': agent.critic2.state_dict(),\n",
    "        'episode': episode,\n",
    "        'learning_rate': learning_rate\n",
    "    }, path)\n",
    "    return path\n",
    "\n",
    "def train_agent(env_name=\"BipedalWalker-v3\", max_episodes=1000, max_steps=1000, device=\"cpu\", render=False, learning_rate=3e-4, updates_per_step=1, start_steps=10000, num_envs=1, alternating_legs_scale=5.0, save_interval=10):\n",
    "    # Setup logger\n",
    "    logger = setup_logger()\n",
    "    logger.info(f\"Starting training with device: {device}, LR: {learning_rate}, Num Envs: {num_envs}\")\n",
    "    \n",
    "    # Determine render mode\n",
    "    render_mode = \"rgb_array\" if render else None\n",
    "    \n",
    "    # Track per-environment progress\n",
    "    env_step_counts = np.zeros(num_envs, dtype=int)\n",
    "    env_episode_counts = np.zeros(num_envs, dtype=int)\n",
    "    \n",
    "    # Create environment\n",
    "    if num_envs > 1:\n",
    "        wrapper_cls = functools.partial(AlternatingLegsRewardWrapper, scale=alternating_legs_scale)\n",
    "        vec_mode = \"sync\" if render else \"async\"\n",
    "        env = gym.make_vec(env_name, num_envs=num_envs, vectorization_mode=vec_mode, wrappers=[wrapper_cls], render_mode=render_mode)\n",
    "        logger.info(f\"Using {num_envs} vectorized environments ({vec_mode}) with AlternatingLegsRewardWrapper (scale={alternating_legs_scale})\")\n",
    "    else:\n",
    "        env = gym.make(env_name, render_mode=render_mode)\n",
    "        env = AlternatingLegsRewardWrapper(env, scale=alternating_legs_scale)\n",
    "        logger.info(f\"Using AlternatingLegsRewardWrapper with scale={alternating_legs_scale}\")\n",
    "        \n",
    "    if num_envs > 1:\n",
    "        state_dim = env.single_observation_space.shape[0]\n",
    "        action_dim = env.single_action_space.shape[0]\n",
    "        action_scale = float(env.single_action_space.high[0])\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        action_scale = float(env.action_space.high[0])\n",
    "    \n",
    "    # Initialize agent\n",
    "    logger.info(f\"Initializing SAC Agent on device: {device}\")\n",
    "    agent = SACAgent(state_dim, action_dim, action_scale, device=device, learning_rate=learning_rate)\n",
    "    \n",
    "    # Training loop\n",
    "    total_steps = 0\n",
    "    episode_rewards = []\n",
    "    \n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    \n",
    "    # Progress bar\n",
    "    pbar = tqdm(range(max_episodes), desc=f\"Training Progress\", unit=\"ep\")\n",
    "    \n",
    "    current_episode = 0\n",
    "    \n",
    "    # Reset env\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    try:\n",
    "        while current_episode < max_episodes:\n",
    "            episode_reward = 0 \n",
    "            if num_envs > 1:\n",
    "                current_rewards = np.zeros(num_envs)\n",
    "                \n",
    "            for step in range(max_steps):\n",
    "                # Select action\n",
    "                if total_steps < start_steps:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    if num_envs > 1:\n",
    "                        action = agent.select_action(state, deterministic=False) \n",
    "                    else:\n",
    "                        action = agent.select_action(state)\n",
    "                \n",
    "                # Take step\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                \n",
    "                # Rendering logic\n",
    "                if render:\n",
    "                    frames = env.render()\n",
    "                    \n",
    "                    # Update counters\n",
    "                    for i in range(num_envs):\n",
    "                        if num_envs > 1:\n",
    "                            is_done = done[i] or truncated[i]\n",
    "                        else:\n",
    "                            is_done = done or truncated\n",
    "                        \n",
    "                        if is_done:\n",
    "                            env_episode_counts[i] += 1\n",
    "                            env_step_counts[i] = 0\n",
    "                        else:\n",
    "                            env_step_counts[i] += 1\n",
    "                            \n",
    "                    # Display frames\n",
    "                    if isinstance(frames, (tuple, list)) and len(frames) == num_envs:\n",
    "                        for i, frame in enumerate(frames):\n",
    "                            # Convert RGB to BGR for OpenCV\n",
    "                            if isinstance(frame, np.ndarray):\n",
    "                                bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                                \n",
    "                                # Add text overlay\n",
    "                                text_info = [\n",
    "                                    f\"Unit: {i}\",\n",
    "                                    f\"Ep: {env_episode_counts[i]}\",\n",
    "                                    f\"Step: {env_step_counts[i]}\"\n",
    "                                ]\n",
    "                                \n",
    "                                for j, line in enumerate(text_info):\n",
    "                                    cv2.putText(bgr_frame, line, (10, 30 + j*25), \n",
    "                                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                                \n",
    "                                cv2.imshow(f\"Unit {i}\", bgr_frame)\n",
    "                        \n",
    "                        cv2.waitKey(1)\n",
    "                    elif isinstance(frames, np.ndarray) and num_envs == 1:\n",
    "                         # Handle single env case if it returns single frame array\n",
    "                         bgr_frame = cv2.cvtColor(frames, cv2.COLOR_RGB2BGR)\n",
    "                         text_info = [\n",
    "                            f\"Unit: 0\",\n",
    "                            f\"Ep: {env_episode_counts[0]}\",\n",
    "                            f\"Step: {env_step_counts[0]}\"\n",
    "                         ]\n",
    "                         for j, line in enumerate(text_info):\n",
    "                            cv2.putText(bgr_frame, line, (10, 30 + j*25), \n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                         cv2.imshow(\"Unit 0\", bgr_frame)\n",
    "                         cv2.waitKey(1)\n",
    "                \n",
    "                # Handle done/truncated\n",
    "                if num_envs > 1:\n",
    "                    done_flag = done | truncated\n",
    "                    current_rewards += reward\n",
    "                    \n",
    "                    for i in range(num_envs):\n",
    "                        if done_flag[i]:\n",
    "                            episode_rewards.append(current_rewards[i])\n",
    "                            current_rewards[i] = 0\n",
    "                            current_episode += 1\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                            # Checkpoint logic inside loop to catch all episodes\n",
    "                            if current_episode % save_interval == 0:\n",
    "                                path = save_checkpoint(agent, current_episode, learning_rate)\n",
    "                                logger.info(f\"Checkpoint saved at episode {current_episode}: {path}\")\n",
    "                                \n",
    "                            if current_episode >= max_episodes:\n",
    "                                break\n",
    "                    \n",
    "                    # Buffer addition for vec env\n",
    "                    real_next_states = next_state.copy()\n",
    "                    if \"_final_observation\" in info:\n",
    "                        mask = info[\"_final_observation\"]\n",
    "                        for i, is_final in enumerate(mask):\n",
    "                            if is_final and \"final_observation\" in info:\n",
    "                                real_next_states[i] = info[\"final_observation\"][i]\n",
    "                    \n",
    "                    agent.replay_buffer.add(state, action, reward, real_next_states, done_flag)\n",
    "                    \n",
    "                else:\n",
    "                    done_flag = done or truncated\n",
    "                    agent.replay_buffer.add(state, action, reward, next_state, done_flag)\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    if done_flag:\n",
    "                        episode_rewards.append(episode_reward)\n",
    "                        current_episode += 1\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                        # Checkpoint logic\n",
    "                        if current_episode % save_interval == 0:\n",
    "                             path = save_checkpoint(agent, current_episode, learning_rate)\n",
    "                             logger.info(f\"Checkpoint saved at episode {current_episode}: {path}\")\n",
    "                        \n",
    "                        state, _ = env.reset()\n",
    "                        break\n",
    "                \n",
    "                state = next_state\n",
    "                total_steps += num_envs\n",
    "                \n",
    "                # Update agent\n",
    "                if len(agent.replay_buffer) > agent.batch_size and total_steps >= start_steps:\n",
    "                    for _ in range(updates_per_step * num_envs):\n",
    "                        agent.update()\n",
    "                        \n",
    "            # Logging progress occasionally\n",
    "            if len(episode_rewards) > 0 and current_episode % 10 == 0:\n",
    "                avg_reward = np.mean(episode_rewards[-10:])\n",
    "                pbar.set_postfix({\n",
    "                    'Last Reward': f'{episode_rewards[-1]:.2f}',\n",
    "                    'Avg Reward (10)': f'{avg_reward:.2f}',\n",
    "                    'Steps': total_steps\n",
    "                })\n",
    "                logger.info(f\"Episode {current_episode}: Avg Reward (10) = {avg_reward:.2f}, Total Steps = {total_steps}\")\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Training interrupted by user. Saving emergency checkpoint...\")\n",
    "        path = save_checkpoint(agent, current_episode, learning_rate, filename=f\"emergency_checkpoint_ep{current_episode}.pth\")\n",
    "        logger.info(f\"Emergency checkpoint saved: {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred: {e}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        if render:\n",
    "            cv2.destroyAllWindows()\n",
    "        env.close()\n",
    "        logger.info(\"Training finished/stopped.\")\n",
    "\n",
    "    return episode_rewards, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Run the training loop and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BipedalWalker training on mps...\n",
      "2025-12-26 02:56:19,663 - INFO - Starting training with device: mps, LR: 0.0003, Num Envs: 4\n",
      "2025-12-26 02:56:19,666 - INFO - Using 4 vectorized environments (sync) with AlternatingLegsRewardWrapper (scale=5.0)\n",
      "2025-12-26 02:56:19,666 - INFO - Initializing SAC Agent on device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 02:56:58,338 - INFO - Checkpoint saved at episode 10: logs/checkpoint_ep10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 02:58:51,818 - INFO - Checkpoint saved at episode 20: logs/checkpoint_ep20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 02:59:18,992 - INFO - Checkpoint saved at episode 30: logs/checkpoint_ep30.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-26 02:59:47,791 - INFO - Checkpoint saved at episode 40: logs/checkpoint_ep40.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "# Set render=True to see the agent learning in real-time (Note: this may slow down training)\n",
    "print(f\"Starting BipedalWalker training on {device}...\")\n",
    "\n",
    "# Configurable Hyperparameters\n",
    "MAX_EPISODES = 1000 # Can extend to 2000 if it doesn't converge\n",
    "UPDATES_PER_STEP = 2  # Higher gradient updates per step for sample efficiency\n",
    "NUM_ENVS = 4 # Number of parallel environments (vectorized)\n",
    "\n",
    "rewards, trained_agent = train_agent(\n",
    "    max_episodes=MAX_EPISODES, \n",
    "    device=device, \n",
    "    updates_per_step=UPDATES_PER_STEP,\n",
    "    start_steps=10000,\n",
    "    num_envs=NUM_ENVS,\n",
    "    render=True  # Set to True to visualize training\n",
    ") \n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(rewards)\n",
    "plt.title(\"Training Rewards\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Recording and Evaluation\n",
    "\n",
    "After training, we can record a video of the agent's performance to visually verify its walking ability. This is required for the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "from IPython.display import Video\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def record_video(agent, env_name=\"BipedalWalker-v3\", filename=\"bipedal_walker\", device=\"cpu\"):\n",
    "    # Create environment with render mode\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    \n",
    "    # Wrap environment to record video\n",
    "    # We force record the first episode\n",
    "    video_folder = \"videos\"\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "    env = RecordVideo(env, video_folder=video_folder, name_prefix=filename, episode_trigger=lambda x: True)\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "    \n",
    "    while not (done or truncated):\n",
    "        # Use deterministic policy for evaluation\n",
    "        action = agent.select_action(state, deterministic=True)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    \n",
    "    env.close()\n",
    "    print(f\"Evaluation Run - Total Reward: {total_reward:.2f}\")\n",
    "    \n",
    "    # Find the video file\n",
    "    mp4_files = glob.glob(f\"{video_folder}/{filename}-episode-0.mp4\")\n",
    "    if mp4_files:\n",
    "        print(f\"Video saved to {mp4_files[0]}\")\n",
    "        return mp4_files[0]\n",
    "    return None\n",
    "\n",
    "# Record and display video\n",
    "# Note: You need to have ffmpeg installed for this to work\n",
    "if 'trained_agent' in locals():\n",
    "    video_path = record_video(trained_agent, device=device)\n",
    "    if video_path:\n",
    "        display(Video(video_path, embed=True, html_attributes=\"controls autoplay loop\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ablation_study(device=\"cpu\"):\n",
    "    # Compare different learning rates\n",
    "    learning_rates = [1e-3, 3e-4, 1e-4]\n",
    "    results = {}\n",
    "    \n",
    "    print(\"Starting Ablation Study on Learning Rates...\")\n",
    "    \n",
    "    # Create logs directory if it doesn't exist\n",
    "    os.makedirs(\"logs\", exist_ok=True)\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTesting Learning Rate: {lr}\")\n",
    "        # Run training for fewer episodes for ablation study to save time, or full duration if desired\n",
    "        # Using 500 episodes for ablation study demonstration\n",
    "        ablation_episodes = 500 \n",
    "        rewards, _ = train_agent(max_episodes=ablation_episodes, device=device, learning_rate=lr)\n",
    "        results[f\"lr_{lr}\"] = rewards\n",
    "        \n",
    "        # Save intermediate result\n",
    "        np.save(f\"logs/ablation_rewards_lr_{lr}.npy\", rewards)\n",
    "    \n",
    "    print(\"Ablation study completed. Plotting results...\")\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for name, rewards in results.items():\n",
    "        # Smooth rewards for better visualization\n",
    "        window = 10\n",
    "        smoothed_rewards = [np.mean(rewards[max(0, i-window):i+1]) for i in range(len(rewards))]\n",
    "        plt.plot(smoothed_rewards, label=name)\n",
    "        \n",
    "    plt.title(\"Ablation Study: Learning Rates (Smoothed)\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Run the ablation study\n",
    "run_ablation_study(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AISF Application Writeup\n",
    "\n",
    "## Applicant Information\n",
    "- **Name**: [Your Name]\n",
    "- **Email**: [Your Email]\n",
    "\n",
    "## Prior Experience\n",
    "*Briefly describe any prior experience you have in RL, Deep Learning, and coding.*\n",
    "\n",
    "## Time Breakdown\n",
    "*Total hours spent: X hours*\n",
    "- Research/Reading: X hours\n",
    "- Coding: X hours\n",
    "- Training/Tuning: X hours\n",
    "- Writing: X hours\n",
    "\n",
    "## Compute Resources\n",
    "*Describe the hardware used.*\n",
    "- CPU: [Number of threads, Model]\n",
    "- GPU: [Model] (if applicable)\n",
    "\n",
    "## Techniques Used\n",
    "*List the techniques used (e.g., SAC, Double Q-Learning, Soft Updates) and justify why they are suitable for this environment.*\n",
    "\n",
    "## Ablation Studies\n",
    "*Summarize the results of your ablation studies here. What hyperparameters did you tweak? What was the impact?*\n",
    "\n",
    "## Discussion of Issues\n",
    "*Discuss challenges encountered, ideas that didn't work, and how you pivoted.*\n",
    "\n",
    "## Conclusion\n",
    "*Summarize what worked well and what could be improved with more time.*\n",
    "\n",
    "## Citations\n",
    "- [Haarnoja, T., et al. \"Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\" arXiv preprint arXiv:1801.01290 (2018).](https://arxiv.org/abs/1801.01290)\n",
    "- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
