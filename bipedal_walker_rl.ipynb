{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Actor-Critic (SAC) for BipedalWalker-v3\n",
    "\n",
    "This notebook implements a Soft Actor-Critic (SAC) agent to solve the BipedalWalker-v3 environment from Gymnasium.\n",
    "SAC is an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install swig\n",
    "!pip install gymnasium[box2d]\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import cv2  # Added for parallel window rendering\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Change to standard tqdm to avoid notebook widget errors\n",
    "from tqdm import tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# For local training on Mac, \n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    try:\n",
    "        torch.cuda.set_device(0)\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"CUDA GPU: {gpu_name} | Memory: {props.total_memory/1024**3:.1f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU info not available: {e}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obstacles Environment Wrapper\n",
    "\n",
    "Enhanced BipedalWalker with procedurally generated obstacles (platforms, gaps, slopes) to increase difficulty and encourage adaptive gait strategies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from Box2D.b2 import (\n",
    "    polygonShape,\n",
    "    edgeShape,\n",
    ")\n",
    "\n",
    "class ObstacleBipedalWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    BipedalWalker with obstacles generated directly on the terrain surface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, difficulty=0.5, seed=None):\n",
    "        super().__init__(env)\n",
    "        self.difficulty = difficulty\n",
    "        self.seed = seed\n",
    "        self.world = None\n",
    "        self.terrain_poly = None \n",
    "        self.obstacle_bodies = []\n",
    "        self.obstacle_polys = []\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Core lifecycle\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        # We must reset the base env first to generate the terrain_y array\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "\n",
    "        self._extract_world()\n",
    "        self._clear_obstacles()\n",
    "        self._spawn_obstacle_course()\n",
    "\n",
    "        return obs, info\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "\n",
    "    def render(self):\n",
    "        mode = getattr(self.env.unwrapped, \"render_mode\", None)\n",
    "        base_frame = self.env.render()\n",
    "        \n",
    "        viewer = getattr(self.env.unwrapped, \"viewer\", None)\n",
    "        if viewer is None:\n",
    "            return base_frame\n",
    "\n",
    "        self._draw_obstacles_to_viewer(viewer)\n",
    "\n",
    "        if mode == \"rgb_array\":\n",
    "            return viewer.render(return_rgb_array=True)\n",
    "        else:\n",
    "            viewer.render()\n",
    "            return base_frame\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # World & cleanup\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    def _extract_world(self):\n",
    "        self.world = self.env.unwrapped.world\n",
    "        self.terrain_poly = getattr(self.env.unwrapped, \"terrain_poly\", None)\n",
    "\n",
    "    def _clear_obstacles(self):\n",
    "        if self.world is not None:\n",
    "            for body in self.obstacle_bodies:\n",
    "                try:\n",
    "                    self.world.DestroyBody(body)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            self.obstacle_bodies.clear()\n",
    "\n",
    "        if self.terrain_poly is not None and self.obstacle_polys:\n",
    "            for poly in self.obstacle_polys:\n",
    "                try:\n",
    "                    self.terrain_poly.remove(poly)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            self.obstacle_polys.clear()\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # Obstacle generation\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    def _spawn_obstacle_course(self):\n",
    "        \"\"\"Generates objects that sit exactly on the terrain surface.\"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        # BipedalWalker terrain starts at x=0. \n",
    "        # We start placing obstacles after the initial flat zone.\n",
    "        x_pos = 10.0 \n",
    "        spacing = 8.0 - (2.0 * self.difficulty)\n",
    "        n_obs = int(5 + 5 * self.difficulty)\n",
    "\n",
    "        for _ in range(n_obs):\n",
    "            # Randomize object dimensions\n",
    "            w = 0.4 + np.random.rand() * 0.4\n",
    "            h = 0.4 + np.random.rand() * 0.8\n",
    "            \n",
    "            self._create_object_on_surface(x_pos, width=w, height=h)\n",
    "            x_pos += spacing\n",
    "\n",
    "    def _create_object_on_surface(self, x_pos, width, height):\n",
    "        \"\"\"Finds terrain height at x_pos and places a static box there.\"\"\"\n",
    "        unwrapped = self.env.unwrapped\n",
    "        \n",
    "        # TERRAIN_STEP is usually 0.1 in BipedalWalker\n",
    "        step = 0.1 \n",
    "        idx = int(x_pos / step)\n",
    "        \n",
    "        # Ensure we are within the bounds of the generated terrain\n",
    "        if idx >= len(unwrapped.terrain_y):\n",
    "            return\n",
    "            \n",
    "        y_surface = unwrapped.terrain_y[idx]\n",
    "\n",
    "        # Box2D uses the center of the shape for position.\n",
    "        # To rest on the surface: center_y = surface_y + (height / 2)\n",
    "        body = self.world.CreateStaticBody(\n",
    "            position=(x_pos, y_surface + height / 2)\n",
    "        )\n",
    "        \n",
    "        shape = polygonShape(box=(width / 2, height / 2))\n",
    "        body.CreateFixture(shape=shape, friction=1.0)\n",
    "        self.obstacle_bodies.append(body)\n",
    "\n",
    "        # Create the visual polygon for the renderer\n",
    "        hw, hh = width / 2, height / 2\n",
    "        vertices = [\n",
    "            (x_pos - hw, y_surface),\n",
    "            (x_pos + hw, y_surface),\n",
    "            (x_pos + hw, y_surface + height),\n",
    "            (x_pos - hw, y_surface + height),\n",
    "        ]\n",
    "        \n",
    "        # Use a distinct 'hazard' color (Reddish)\n",
    "        self._register_render_poly(vertices, color=(0.8, 0.3, 0.3))\n",
    "\n",
    "    def _register_render_poly(self, vertices, color):\n",
    "        if self.terrain_poly is None:\n",
    "            return\n",
    "        poly = (vertices, color)\n",
    "        self.terrain_poly.append(poly)\n",
    "        self.obstacle_polys.append(poly)\n",
    "\n",
    "    def _draw_obstacles_to_viewer(self, viewer):\n",
    "        \"\"\"Direct drawing for environments with active viewers.\"\"\"\n",
    "        for body in self.obstacle_bodies:\n",
    "            for fixture in body.fixtures:\n",
    "                shape = fixture.shape\n",
    "                if isinstance(shape, polygonShape):\n",
    "                    verts = [body.transform * v for v in shape.vertices]\n",
    "                    # Draw a slightly darker border for visibility\n",
    "                    viewer.draw_polygon(verts, color=(0.5, 0.2, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "The replay buffer stores experience tuples (state, action, reward, next_state, done) to be sampled during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, state_dim, action_dim, buffer_size=int(1e6)):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "        \n",
    "        self.state = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
    "        self.action = np.zeros((buffer_size, action_dim), dtype=np.float32)\n",
    "        self.reward = np.zeros((buffer_size, 1), dtype=np.float32)\n",
    "        self.next_state = np.zeros((buffer_size, state_dim), dtype=np.float32)\n",
    "        self.done = np.zeros((buffer_size, 1), dtype=np.float32)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # Check if input is batch or single\n",
    "        if state.ndim == 1:\n",
    "            state = state[None, :]\n",
    "            action = action[None, :]\n",
    "            reward = np.array(reward)[None]\n",
    "            next_state = next_state[None, :]\n",
    "            done = np.array(done)[None]\n",
    "        \n",
    "        batch_size = len(state)\n",
    "        \n",
    "        if self.ptr + batch_size <= self.buffer_size:\n",
    "            self.state[self.ptr:self.ptr+batch_size] = state\n",
    "            self.action[self.ptr:self.ptr+batch_size] = action\n",
    "            self.reward[self.ptr:self.ptr+batch_size] = reward.reshape(-1, 1)\n",
    "            self.next_state[self.ptr:self.ptr+batch_size] = next_state\n",
    "            self.done[self.ptr:self.ptr+batch_size] = done.reshape(-1, 1)\n",
    "            self.ptr = (self.ptr + batch_size) % self.buffer_size\n",
    "        else:\n",
    "            # Handle wrap around\n",
    "            overflow = (self.ptr + batch_size) - self.buffer_size\n",
    "            split = batch_size - overflow\n",
    "            \n",
    "            # First part\n",
    "            self.state[self.ptr:] = state[:split]\n",
    "            self.action[self.ptr:] = action[:split]\n",
    "            self.reward[self.ptr:] = reward[:split].reshape(-1, 1)\n",
    "            self.next_state[self.ptr:] = next_state[:split]\n",
    "            self.done[self.ptr:] = done[:split].reshape(-1, 1)\n",
    "            \n",
    "            # Second part (overflow)\n",
    "            self.state[:overflow] = state[split:]\n",
    "            self.action[:overflow] = action[split:]\n",
    "            self.reward[:overflow] = reward[split:].reshape(-1, 1)\n",
    "            self.next_state[:overflow] = next_state[split:]\n",
    "            self.done[:overflow] = done[split:].reshape(-1, 1)\n",
    "            self.ptr = overflow\n",
    "            \n",
    "        self.size = min(self.size + batch_size, self.buffer_size)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        return (\n",
    "            self.state[ind],\n",
    "            self.action[ind],\n",
    "            self.reward[ind],\n",
    "            self.next_state[ind],\n",
    "            self.done[ind]\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architectures\n",
    "\n",
    "We define the Actor and Critic networks. The Actor outputs the mean and log standard deviation of the action distribution. The Critic estimates the Q-value for a given state-action pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, action_dim)\n",
    "        # Output log_std from the network instead of a fixed parameter\n",
    "        self.log_std_linear = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        # Constrain log_std for numerical stability\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        return mu, log_std\n",
    "    \n",
    "    def sample(self, state):\n",
    "        mu, log_std = self.forward(state)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mu, std)\n",
    "        x_t = dist.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        \n",
    "        # Log prob calculation\n",
    "        log_prob = dist.log_prob(x_t)\n",
    "        log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        # Critic takes state and action as input\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.q_value = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.q_value(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAC Agent\n",
    "\n",
    "The SAC agent orchestrates the interaction with the environment and the training process. It maintains the actor, two critics (for double Q-learning), and their target networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        action_scale=1.0,\n",
    "        device=\"cpu\",\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        tau=0.001,\n",
    "        alpha=0.2,\n",
    "        batch_size=1024,\n",
    "        buffer_size=int(1e6),\n",
    "        target_entropy=None,\n",
    "        hidden_dim=256,\n",
    "    ):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_scale = action_scale\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau  # Soft target update rate\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        # Automatic entropy tuning target: default to -dim(A) unless overridden\n",
    "        self.target_entropy = target_entropy if target_entropy is not None else -float(action_dim)\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = ActorNetwork(state_dim, action_dim, hidden_dim=hidden_dim).to(self.device)\n",
    "        self.critic1 = CriticNetwork(state_dim, action_dim, hidden_dim=hidden_dim).to(self.device)\n",
    "        self.critic2 = CriticNetwork(state_dim, action_dim, hidden_dim=hidden_dim).to(self.device)\n",
    "        self.target_critic1 = CriticNetwork(state_dim, action_dim, hidden_dim=hidden_dim).to(self.device)\n",
    "        self.target_critic2 = CriticNetwork(state_dim, action_dim, hidden_dim=hidden_dim).to(self.device)\n",
    "        \n",
    "        # Copy weights to target networks\n",
    "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=self.learning_rate)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=self.learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # Replay buffer (Now using optimized Numpy buffer with batch support)\n",
    "        self.replay_buffer = ReplayBuffer(state_dim, action_dim, self.buffer_size)\n",
    "        \n",
    "        # Log alpha for entropy adjustment\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=self.learning_rate)\n",
    "        \n",
    "    def select_action(self, state, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            state = np.array(state)\n",
    "            if state.ndim == 1:\n",
    "                state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            else:\n",
    "                state_t = torch.FloatTensor(state).to(self.device)\n",
    "                \n",
    "            if deterministic:\n",
    "                mu, _ = self.actor(state_t)\n",
    "                action = torch.tanh(mu).cpu().numpy()\n",
    "            else:\n",
    "                action, _ = self.actor.sample(state_t)\n",
    "                action = action.cpu().numpy()\n",
    "            \n",
    "            # If we passed a single state (ndim=1), we want a single action (ndim=1)\n",
    "            if state.ndim == 1:\n",
    "                return action.flatten()\n",
    "            \n",
    "            return action\n",
    "    \n",
    "    def update(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "            \n",
    "        # Sample from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
    "        dones = torch.FloatTensor(dones).to(self.device)\n",
    "        \n",
    "        # Update critic\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_pi = self.actor.sample(next_states)\n",
    "            next_q1 = self.target_critic1(next_states, next_actions)\n",
    "            next_q2 = self.target_critic2(next_states, next_actions)\n",
    "            next_q = torch.min(next_q1, next_q2) - self.alpha * next_log_pi\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        current_q1 = self.critic1(states, actions)\n",
    "        current_q2 = self.critic2(states, actions)\n",
    "        \n",
    "        critic1_loss = F.mse_loss(current_q1, target_q)\n",
    "        critic2_loss = F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        # Gradient clipping for critics\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic1.parameters(), 1.0)\n",
    "        self.critic1_optimizer.step()\n",
    "        \n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        # Gradient clipping for critics\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic2.parameters(), 1.0)\n",
    "        self.critic2_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        actions_pred, log_pi = self.actor.sample(states)\n",
    "        q1 = self.critic1(states, actions_pred)\n",
    "        q2 = self.critic2(states, actions_pred)\n",
    "        q = torch.min(q1, q2)\n",
    "        \n",
    "        actor_loss = (self.alpha * log_pi - q).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        # Gradient clipping for actor\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Failsafe for target_entropy\n",
    "        if not hasattr(self, 'target_entropy'):\n",
    "            self.target_entropy = -float(self.action_dim)\n",
    "            \n",
    "        # Update alpha\n",
    "        alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "        \n",
    "        # Update target networks\n",
    "        for param, target_param in zip(self.critic1.parameters(), self.target_critic1.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "            \n",
    "        for param, target_param in zip(self.critic2.parameters(), self.target_critic2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "We train the agent in the environment. We'll also log rewards and save checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import logging\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "\n",
    "# Define a custom handler to work with tqdm\n",
    "class TqdmLoggingHandler(logging.Handler):\n",
    "    def __init__(self, level=logging.NOTSET):\n",
    "        super().__init__(level)\n",
    "\n",
    "    def emit(self, record):\n",
    "        try:\n",
    "            msg = self.format(record)\n",
    "            tqdm.write(msg)\n",
    "            self.flush()\n",
    "        except Exception:\n",
    "            self.handleError(record)\n",
    "\n",
    "def setup_logger(log_dir=\"logs\"):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    logger = logging.getLogger(\"BipedalWalker\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    \n",
    "    # Clear existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "        \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = os.path.join(log_dir, f\"training_{timestamp}.log\")\n",
    "    \n",
    "    # File handler\n",
    "    fh = logging.FileHandler(log_file)\n",
    "    fh.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    \n",
    "    # Console handler using TqdmLoggingHandler\n",
    "    ch = TqdmLoggingHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class WalkingRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def step(self, action):\n",
    "        state, reward, done, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # --- State Mapping for BipedalWalker-v3 ---\n",
    "        hull_angle = state[0]\n",
    "        fwd_vel = state[2]\n",
    "        # Leg 1\n",
    "        hip1_angle, hip1_speed, leg1_contact = state[4], state[5], state[8]\n",
    "        # Leg 2\n",
    "        hip2_angle, hip2_speed, leg2_contact = state[9], state[10], state[13]\n",
    "        \n",
    "        # 1. Energy Penalty (Keep it efficient)\n",
    "        energy_penalty = -0.0005 * np.sum(np.square(action))\n",
    "        \n",
    "        # 2. Stability Reward (Keep the body level)\n",
    "        # Penalize leaning, softened for obstacles\n",
    "        if leg1_contact and leg2_contact:\n",
    "            stability_reward = -0.15 * abs(hull_angle)\n",
    "        else:\n",
    "            stability_reward = -0.05 * abs(hull_angle)\n",
    "        \n",
    "        # 3. Advanced Gait (Scissoring) Reward\n",
    "        gait_reward = 0\n",
    "        if fwd_vel > 0.05:\n",
    "\n",
    "            stride_width = abs(hip1_angle - hip2_angle)\n",
    "\n",
    "            coordination = np.maximum(0, -hip1_speed * hip2_speed)\n",
    "\n",
    "            contact_bonus = 0.2 if leg1_contact != leg2_contact else -0.1\n",
    "\n",
    "            scissor_penalty = -0.3 * np.maximum(0.0, hip1_angle * hip2_angle)\n",
    "\n",
    "            same_vel_penalty = -0.2 * np.maximum(0.0, hip1_speed * hip2_speed)\n",
    "\n",
    "            hip_action_corr = action[0] * action[2]\n",
    "            action_scissor_penalty = -0.1 * np.maximum(0.0, hip_action_corr)\n",
    "\n",
    "            gait_reward = (\n",
    "                0.4 * np.tanh(stride_width)\n",
    "                + 0.4 * np.tanh(coordination)\n",
    "                + contact_bonus\n",
    "                + scissor_penalty\n",
    "                + same_vel_penalty\n",
    "                + action_scissor_penalty\n",
    "            )\n",
    "\n",
    "        # 4. Anti-Crouch Reward (Optional but helpful)\n",
    "        # state[1] is the hull Y-position. Standard height is ~1.0. \n",
    "        # Penalize if the agent tries to \"crawl\" on its knees.\n",
    "        crouch_penalty = 0\n",
    "        if state[1] < 0.8:\n",
    "            crouch_penalty = -0.2\n",
    "\n",
    "        # Apply the shaped rewards\n",
    "        reward += energy_penalty + stability_reward + gait_reward + crouch_penalty\n",
    "        \n",
    "        return state, reward, done, truncated, info\n",
    "\n",
    "def save_checkpoint(agent, episode, learning_rate, log_dir=\"logs\", filename=None):\n",
    "    if filename is None:\n",
    "        filename = f\"checkpoint_ep{episode}.pth\"\n",
    "    \n",
    "    path = os.path.join(log_dir, filename)\n",
    "    torch.save({\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic1_state_dict': agent.critic1.state_dict(),\n",
    "        'critic2_state_dict': agent.critic2.state_dict(),\n",
    "        'episode': episode,\n",
    "        'learning_rate': learning_rate\n",
    "    }, path)\n",
    "    return path\n",
    "\n",
    "# CSV logging of model parameters per episode\n",
    "\n",
    "def write_params_csv(agent, episode, csv_path):\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    write_header = not os.path.exists(csv_path)\n",
    "    with open(csv_path, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow([\"timestamp\", \"episode\", \"component\", \"parameter\", \"shape\", \"values\"])\n",
    "\n",
    "        ts = datetime.now().isoformat(timespec=\"seconds\")\n",
    "\n",
    "        def _dump_state_dict(state_dict, component):\n",
    "            for name, tensor in state_dict.items():\n",
    "                arr = tensor.detach().cpu().numpy()\n",
    "                flat = arr.reshape(-1).tolist()\n",
    "                writer.writerow([\n",
    "                    ts,\n",
    "                    episode,\n",
    "                    component,\n",
    "                    name,\n",
    "                    list(arr.shape),\n",
    "                    \" \".join(map(str, flat))\n",
    "                ])\n",
    "\n",
    "        _dump_state_dict(agent.actor.state_dict(), \"actor\")\n",
    "        _dump_state_dict(agent.critic1.state_dict(), \"critic1\")\n",
    "        _dump_state_dict(agent.critic2.state_dict(), \"critic2\")\n",
    "        writer.writerow([\n",
    "            ts,\n",
    "            episode,\n",
    "            \"alpha\",\n",
    "            \"log_alpha\",\n",
    "            [1],\n",
    "            str(agent.log_alpha.detach().cpu().item())\n",
    "        ])\n",
    "\n",
    "# CSV logging of rewards and training metrics per episode\n",
    "\n",
    "def write_rewards_csv(agent, episode, total_reward, total_steps, learning_rate, num_envs, csv_path, avg10=None, avg100=None, log_dir_name=None):\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    write_header = not os.path.exists(csv_path)\n",
    "    # Compute lightweight stats\n",
    "    alpha = float(agent.log_alpha.detach().cpu().exp().item())\n",
    "    actor_norm = float(sum(p.data.norm().item() for p in agent.actor.parameters()))\n",
    "    critic1_norm = float(sum(p.data.norm().item() for p in agent.critic1.parameters()))\n",
    "    critic2_norm = float(sum(p.data.norm().item() for p in agent.critic2.parameters()))\n",
    "\n",
    "    with open(csv_path, \"a\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        if write_header:\n",
    "            writer.writerow([\n",
    "                \"timestamp\", \"episode\", \"reward\", \"avg10\", \"avg100\", \"total_steps\",\n",
    "                \"learning_rate\", \"num_envs\", \"alpha\",\n",
    "                \"actor_param_norm\", \"critic1_param_norm\", \"critic2_param_norm\", \"log_dir\"\n",
    "            ])\n",
    "        ts = datetime.now().isoformat(timespec=\"seconds\")\n",
    "        writer.writerow([\n",
    "            ts, episode, float(total_reward),\n",
    "            float(avg10) if avg10 is not None else \"\",\n",
    "            float(avg100) if avg100 is not None else \"\",\n",
    "            int(total_steps),\n",
    "            float(learning_rate), int(num_envs), float(alpha),\n",
    "            float(actor_norm), float(critic1_norm), float(critic2_norm),\n",
    "            log_dir_name or \"\"\n",
    "        ])\n",
    "\n",
    "\n",
    "def train_agent(\n",
    "    env_name=\"BipedalWalker-v3\",\n",
    "    max_episodes=1000,\n",
    "    max_steps=1000,\n",
    "    device=\"cpu\",\n",
    "    render_freq=50,\n",
    "    learning_rate=3e-4,\n",
    "    updates_per_step=1,\n",
    "    start_steps=20000,\n",
    "    num_envs=1,\n",
    "    save_interval=10,\n",
    "    log_dir=None,\n",
    "    use_obstacles=False,\n",
    "    obstacle_difficulty=1.0,\n",
    "    seed=42,\n",
    "    tau=0.001,\n",
    "    batch_size=1024,\n",
    "    buffer_size=int(1e6),\n",
    "    target_entropy=None,\n",
    "    hidden_dim=256,\n",
    "):\n",
    "    # Set seeds for reproducibility per run\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Determine log directory\n",
    "    if log_dir is None:\n",
    "        log_dir = os.path.join(\"logs\", datetime.now().strftime(\"run_%Y%m%d_%H%M%S\"))\n",
    "    \n",
    "    # Add obstacle info to log directory if enabled\n",
    "    if use_obstacles:\n",
    "        log_dir = log_dir.replace(\"run_\", f\"run_obstacles_d{obstacle_difficulty}_\")\n",
    "\n",
    "    # Prepare CSV paths for logging\n",
    "    params_csv_path = os.path.join(log_dir, \"params.csv\")\n",
    "    rewards_csv_path = os.path.join(log_dir, \"rewards.csv\")\n",
    "\n",
    "    # Setup logger\n",
    "    logger = setup_logger(log_dir=log_dir)\n",
    "    logger.info(f\"Starting training with device: {device}, LR: {learning_rate}, Num Envs: {num_envs}\")\n",
    "    logger.info(f\"Hyperparams: tau={tau}, batch_size={batch_size}, buffer_size={buffer_size}, target_entropy={target_entropy}, hidden_dim={hidden_dim}, seed={seed}\")\n",
    "    logger.info(f\"Obstacles enabled: {use_obstacles}, Difficulty: {obstacle_difficulty if use_obstacles else 'N/A'}\")\n",
    "    logger.info(f\"Logs and checkpoints will be saved to: {log_dir}\")\n",
    "    \n",
    "    # Determine render mode\n",
    "    render_mode = \"rgb_array\"\n",
    "    \n",
    "    # Track per-environment progress\n",
    "    env_step_counts = np.zeros(num_envs, dtype=int)\n",
    "    env_episode_counts = np.zeros(num_envs, dtype=int)\n",
    "    \n",
    "    # Create environment\n",
    "    if num_envs > 1:\n",
    "        vec_mode = \"async\"\n",
    "        wrappers = [WalkingRewardWrapper]\n",
    "        if use_obstacles:\n",
    "            wrappers.append(lambda env: ObstacleBipedalWrapper(env, difficulty=obstacle_difficulty))\n",
    "        env = gym.make_vec(env_name, num_envs=num_envs, vectorization_mode=vec_mode, wrappers=wrappers, render_mode=render_mode)\n",
    "        logger.info(f\"Using {num_envs} vectorized environments ({vec_mode}) with WalkingRewardWrapper\")\n",
    "        if use_obstacles:\n",
    "            logger.info(f\"Obstacle environment enabled with difficulty={obstacle_difficulty}\")\n",
    "    else:\n",
    "        env = gym.make(env_name, render_mode=render_mode)\n",
    "        env = WalkingRewardWrapper(env)\n",
    "        if use_obstacles:\n",
    "            env = ObstacleBipedalWrapper(env, difficulty=obstacle_difficulty)\n",
    "            logger.info(f\"Using ObstacleBipedalWrapper with difficulty={obstacle_difficulty}\")\n",
    "        logger.info(\"Using WalkingRewardWrapper\")\n",
    "        \n",
    "    if num_envs > 1:\n",
    "        state_dim = env.single_observation_space.shape[0]\n",
    "        action_dim = env.single_action_space.shape[0]\n",
    "        action_scale = float(env.single_action_space.high[0])\n",
    "    else:\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.shape[0]\n",
    "        action_scale = float(env.action_space.high[0])\n",
    "    \n",
    "    # Initialize agent\n",
    "    logger.info(f\"Initializing SAC Agent on device: {device}\")\n",
    "    agent = SACAgent(\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        action_scale,\n",
    "        device=device,\n",
    "        learning_rate=learning_rate,\n",
    "        gamma=0.99,\n",
    "        tau=tau,\n",
    "        alpha=0.2,\n",
    "        batch_size=batch_size,\n",
    "        buffer_size=buffer_size,\n",
    "        target_entropy=target_entropy,\n",
    "        hidden_dim=hidden_dim,\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    total_steps = 0\n",
    "    episode_rewards = []\n",
    "    recent_rewards = deque(maxlen=100)\n",
    "    \n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    pbar = tqdm(range(max_episodes), desc=f\"Training Progress\", unit=\"ep\")\n",
    "    \n",
    "    current_episode = 0\n",
    "    training_complete = False\n",
    "    \n",
    "    # Reset env\n",
    "    state, _ = env.reset()\n",
    "    \n",
    "    try:\n",
    "        while current_episode < max_episodes and not training_complete:\n",
    "            episode_reward = 0 \n",
    "            if num_envs > 1:\n",
    "                current_rewards = np.zeros(num_envs)\n",
    "                \n",
    "            # Determine if we should render this episode\n",
    "            should_render = (render_freq > 0) and (current_episode % render_freq == 0)\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                # Select action\n",
    "                if total_steps < start_steps:\n",
    "                    if num_envs > 1:\n",
    "                        action = np.array([env.single_action_space.sample() for _ in range(num_envs)])\n",
    "                    else:\n",
    "                        action = env.action_space.sample()\n",
    "                else:\n",
    "                    if num_envs > 1:\n",
    "                        action = agent.select_action(state, deterministic=False) \n",
    "                    else:\n",
    "                        action = agent.select_action(state)\n",
    "                        if isinstance(action, np.ndarray) and action.ndim > 1:\n",
    "                            action = action.flatten()\n",
    "                \n",
    "                # Take step\n",
    "                next_state, reward, done, truncated, info = env.step(action)\n",
    "                \n",
    "                # Rendering logic (only for single environment)\n",
    "                if should_render and num_envs == 1:\n",
    "                    try:\n",
    "                        frame = env.render()\n",
    "                        if isinstance(frame, np.ndarray):\n",
    "                            bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "                            cv2.putText(bgr_frame, f\"Ep: {current_episode}\", (10, 30), \n",
    "                                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                            cv2.imshow(\"BipedalWalker Training\", bgr_frame)\n",
    "                            cv2.waitKey(1)\n",
    "                    except Exception as e:\n",
    "                        # Rendering not available or window closed\n",
    "                        pass\n",
    "                elif should_render and num_envs > 1 and step == 0:\n",
    "                    # Log once per episode that rendering is disabled for parallel envs\n",
    "                    logger.info(f\"Rendering disabled for parallel environments (num_envs={num_envs})\")\n",
    "                \n",
    "                # Handle done/truncated\n",
    "                if num_envs > 1:\n",
    "                    done_flag = done | truncated\n",
    "                    current_rewards += reward\n",
    "                    \n",
    "                    for i in range(num_envs):\n",
    "                        if done_flag[i]:\n",
    "                            episode_rewards.append(current_rewards[i])\n",
    "                            recent_rewards.append(current_rewards[i])\n",
    "                            current_rewards[i] = 0\n",
    "                            current_episode += 1\n",
    "                            pbar.update(1)\n",
    "                            \n",
    "                            # Update progress bar\n",
    "                            avg_reward = np.mean(recent_rewards) if len(recent_rewards) > 0 else 0.0\n",
    "                            pbar.set_postfix({\n",
    "                                'Last': f'{episode_rewards[-1]:.1f}',\n",
    "                                'Avg': f'{avg_reward:.1f}',\n",
    "                                'Steps': total_steps\n",
    "                            })\n",
    "\n",
    "                            # Write rewards CSV for this episode\n",
    "                            avg10 = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "                            write_rewards_csv(\n",
    "                                agent=agent,\n",
    "                                episode=current_episode,\n",
    "                                total_reward=episode_rewards[-1],\n",
    "                                total_steps=total_steps,\n",
    "                                learning_rate=learning_rate,\n",
    "                                num_envs=num_envs,\n",
    "                                csv_path=rewards_csv_path,\n",
    "                                avg10=avg10,\n",
    "                                avg100=avg_reward,\n",
    "                                log_dir_name=os.path.basename(log_dir)\n",
    "                            )\n",
    "\n",
    "                            # Update render status\n",
    "                            should_render = (render_freq > 0) and (current_episode % render_freq == 0)\n",
    "                            \n",
    "                            # Checkpoint logic\n",
    "                            if current_episode % save_interval == 0:\n",
    "                                path = save_checkpoint(agent, current_episode, learning_rate, log_dir=log_dir)\n",
    "                                logger.info(f\"Checkpoint saved at episode {current_episode}: {path}\")\n",
    "                                # Append parameters to CSV\n",
    "                                write_params_csv(agent, current_episode, params_csv_path)\n",
    "                                \n",
    "                            if current_episode >= max_episodes:\n",
    "                                training_complete = True\n",
    "                                break\n",
    "                    \n",
    "                    # Buffer addition for vec env\n",
    "                    real_next_states = next_state.copy()\n",
    "                    if \"_final_observation\" in info:\n",
    "                        mask = info[\"_final_observation\"]\n",
    "                        for i, is_final in enumerate(mask):\n",
    "                            if is_final and \"final_observation\" in info:\n",
    "                                real_next_states[i] = info[\"final_observation\"][i]\n",
    "                    \n",
    "                    agent.replay_buffer.add(state, action, reward, real_next_states, done_flag)\n",
    "                    \n",
    "                    # Exit early if we've reached max episodes\n",
    "                    if training_complete:\n",
    "                        break\n",
    "                    \n",
    "                else:\n",
    "                    done_flag = done or truncated\n",
    "                    agent.replay_buffer.add(state, action, reward, next_state, done_flag)\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    if done_flag:\n",
    "                        episode_rewards.append(episode_reward)\n",
    "                        recent_rewards.append(episode_reward)\n",
    "                        current_episode += 1\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                        # Update progress bar\n",
    "                        avg_reward = np.mean(recent_rewards) if len(recent_rewards) > 0 else 0.0\n",
    "                        pbar.set_postfix({\n",
    "                            'Last': f'{episode_rewards[-1]:.1f}',\n",
    "                            'Avg': f'{avg_reward:.1f}',\n",
    "                            'Steps': total_steps\n",
    "                        })\n",
    "\n",
    "                        # Write rewards CSV for this episode\n",
    "                        avg10 = np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else np.mean(episode_rewards)\n",
    "                        write_rewards_csv(\n",
    "                            agent=agent,\n",
    "                            episode=current_episode,\n",
    "                            total_reward=episode_reward,\n",
    "                            total_steps=total_steps,\n",
    "                            learning_rate=learning_rate,\n",
    "                            num_envs=num_envs,\n",
    "                            csv_path=rewards_csv_path,\n",
    "                            avg10=avg10,\n",
    "                            avg100=avg_reward,\n",
    "                            log_dir_name=os.path.basename(log_dir)\n",
    "                        )\n",
    "\n",
    "                        should_render = (render_freq > 0) and (current_episode % render_freq == 0)\n",
    "                        \n",
    "                        # Checkpoint logic\n",
    "                        if current_episode % save_interval == 0:\n",
    "                             path = save_checkpoint(agent, current_episode, learning_rate, log_dir=log_dir)\n",
    "                             logger.info(f\"Checkpoint saved at episode {current_episode}: {path}\")\n",
    "                             # Append parameters to CSV\n",
    "                             write_params_csv(agent, current_episode, params_csv_path)\n",
    "                        \n",
    "                        # Check if we've reached max episodes\n",
    "                        if current_episode >= max_episodes:\n",
    "                            training_complete = True\n",
    "                        \n",
    "                        state, _ = env.reset()\n",
    "                        break\n",
    "                \n",
    "                state = next_state\n",
    "                total_steps += num_envs\n",
    "                \n",
    "                # Update agent\n",
    "                if len(agent.replay_buffer) > agent.batch_size and total_steps >= start_steps:\n",
    "                    for _ in range(updates_per_step * num_envs):\n",
    "                        agent.update()\n",
    "\n",
    "                # Update progress bar steps periodically\n",
    "                if total_steps % 1000 == 0:\n",
    "                    last_r = episode_rewards[-1] if episode_rewards else 0\n",
    "                    avg_r = np.mean(recent_rewards) if len(recent_rewards) > 0 else 0\n",
    "                    pbar.set_postfix({\n",
    "                        'Last': f'{last_r:.1f}',\n",
    "                        'Avg': f'{avg_r:.1f}',\n",
    "                        'Steps': total_steps\n",
    "                    })\n",
    "                        \n",
    "            # Logging progress occasionally\n",
    "            if len(episode_rewards) > 0 and current_episode % 10 == 0:\n",
    "                avg_reward = np.mean(episode_rewards[-10:])\n",
    "                logger.info(f\"Episode {current_episode}: Avg Reward (10) = {avg_reward:.2f}, Total Steps = {total_steps}\")\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        logger.warning(\"Training interrupted by user. Saving emergency checkpoint...\")\n",
    "        path = save_checkpoint(agent, current_episode, learning_rate, log_dir=log_dir, filename=f\"emergency_checkpoint_ep{current_episode}.pth\")\n",
    "        logger.info(f\"Emergency checkpoint saved: {path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred: {e}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        try:\n",
    "            cv2.destroyAllWindows()\n",
    "            cv2.waitKey(1)\n",
    "        except:\n",
    "            pass\n",
    "        env.close()\n",
    "        logger.info(\"Training finished/stopped.\")\n",
    "\n",
    "    return episode_rewards, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Execution\n",
    "\n",
    "Train the agent with flexible options for obstacles, difficulty levels, and hyperparameters. Compare baseline vs obstacle-based environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Starting BipedalWalker training on {device}...\")\n",
    "\n",
    "# Configurable Hyperparameters\n",
    "MAX_EPISODES = 2000  # Episodes per run (reduced for comparison study)\n",
    "UPDATES_PER_STEP = 1\n",
    "NUM_ENVS = 32  # Number of parallel environments\n",
    "START_STEPS = 10000\n",
    "RENDER_FREQ = 0\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 1: Train Baseline (No Obstacles)\n",
    "# ============================================================================\n",
    "\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"TRAINING: Baseline Agent (No Obstacles)\")\n",
    "# print(\"=\"*70)\n",
    "\n",
    "# rewards_baseline, agent_baseline = train_agent(\n",
    "#     max_episodes=MAX_EPISODES, \n",
    "#     device=device, \n",
    "#     updates_per_step=UPDATES_PER_STEP,\n",
    "#     start_steps=START_STEPS,\n",
    "#     num_envs=NUM_ENVS,\n",
    "#     render_freq=RENDER_FREQ,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     use_obstacles=False\n",
    "# )\n",
    "# print(\" Baseline training completed!\")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTION 2: Train with Obstacles\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING: Agent with Obstacles (difficulty=0.7)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "rewards_obstacles, agent_obstacles = train_agent(\n",
    "    max_episodes=MAX_EPISODES, \n",
    "    device=device, \n",
    "    updates_per_step=UPDATES_PER_STEP,\n",
    "    start_steps=START_STEPS,\n",
    "    num_envs=NUM_ENVS,\n",
    "    render_freq=RENDER_FREQ,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    use_obstacles=True,\n",
    "    obstacle_difficulty=0.7\n",
    ")\n",
    "print(\" Obstacle training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results Visualization\n",
    "\n",
    "Visualize training curves and compare performance between baseline and obstacle-based training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RESULTS: Visualization & Comparison\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS: Baseline vs Obstacles Comparison\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Individual training curves\n",
    "window = 30\n",
    "# baseline_smoothed = [np.mean(rewards_baseline[max(0, i-window):i+1]) for i in range(len(rewards_baseline))]\n",
    "baseline_smoothed = [0.0 for _ in range(len(rewards_obstacles))]  # Placeholder since baseline training is commented out\n",
    "obstacles_smoothed = [np.mean(rewards_obstacles[max(0, i-window):i+1]) for i in range(len(rewards_obstacles))]\n",
    "\n",
    "# axes[0].plot(rewards_baseline, alpha=0.2, color='blue', label='Raw (Baseline)')\n",
    "# axes[0].plot(baseline_smoothed, linewidth=2.5, color='blue', label='Smoothed (Baseline)')\n",
    "axes[0].plot(rewards_obstacles, alpha=0.2, color='red', label='Raw (Obstacles)')\n",
    "axes[0].plot(obstacles_smoothed, linewidth=2.5, color='red', label='Smoothed (Obstacles)')\n",
    "axes[0].set_title(\"Training Curves: Baseline vs Obstacles\", fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel(\"Episode\")\n",
    "axes[0].set_ylabel(\"Reward\")\n",
    "axes[0].legend(fontsize=9)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Direct comparison (smoothed only)\n",
    "# axes[1].plot(baseline_smoothed, linewidth=3, color='blue', label='Baseline (No Obstacles)', marker='o', markersize=3, markevery=20)\n",
    "axes[1].plot(obstacles_smoothed, linewidth=3, color='red', label='With Obstacles (difficulty=0.7)', marker='s', markersize=3, markevery=20)\n",
    "axes[1].set_title(\"Smoothed Reward Comparison\", fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel(\"Episode\")\n",
    "axes[1].set_ylabel(\"Smoothed Reward\")\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n{'Metric':<30} {'Baseline':<20} {'With Obstacles':<20}\")\n",
    "print(\"-\" * 70)\n",
    "# print(f\"{'Total Episodes':<30} {len(rewards_baseline):<20} {len(rewards_obstacles):<20}\")\n",
    "# print(f\"{'Best Reward':<30} {max(rewards_baseline):<20.2f} {max(rewards_obstacles):<20.2f}\")\n",
    "# print(f\"{'Worst Reward':<30} {min(rewards_baseline):<20.2f} {min(rewards_obstacles):<20.2f}\")\n",
    "\n",
    "# avg_baseline = np.mean(rewards_baseline[-100:]) if len(rewards_baseline) >= 100 else np.mean(rewards_baseline)\n",
    "avg_baseline = 0.0  # Placeholder since baseline training is commented out\n",
    "avg_obstacles = np.mean(rewards_obstacles[-100:]) if len(rewards_obstacles) >= 100 else np.mean(rewards_obstacles)\n",
    "print(f\"{'Avg (Last 100 eps)':<30} {avg_baseline:<20.2f} {avg_obstacles:<20.2f}\")\n",
    "print(f\"{'Final Smoothed Reward':<30} {baseline_smoothed[-1]:<20.2f} {obstacles_smoothed[-1]:<20.2f}\")\n",
    "\n",
    "# improvement = baseline_smoothed[-1] - obstacles_smoothed[-1]\n",
    "# print(f\"{'Difficulty Gap (B-O)':<30} {improvement:<20.2f}\")\n",
    "# print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Recording and Evaluation\n",
    "\n",
    "After training, we can record a video of the agent's performance to visually verify its walking ability. This is required for the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "from IPython.display import Video\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def load_checkpoint(checkpoint_path, state_dim, action_dim, device=\"cpu\", learning_rate=3e-4):\n",
    "    \"\"\"Load a trained agent from a checkpoint.\"\"\"\n",
    "    agent = SACAgent(state_dim, action_dim, device=device, learning_rate=learning_rate)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    # Load with strict=False to handle architecture changes (old log_std param vs new log_std_linear layer)\n",
    "    agent.actor.load_state_dict(checkpoint['actor_state_dict'], strict=False)\n",
    "    agent.critic1.load_state_dict(checkpoint['critic1_state_dict'], strict=False)\n",
    "    agent.critic2.load_state_dict(checkpoint['critic2_state_dict'], strict=False)\n",
    "\n",
    "    print(f\"Loaded checkpoint from: {checkpoint_path}\")\n",
    "    return agent\n",
    "\n",
    "def record_video(agent, env_name=\"BipedalWalker-v3\", filename=\"bipedal_walker\", device=\"cpu\", use_obstacles=False, obstacle_difficulty=0.5):\n",
    "    # Create environment with render mode\n",
    "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
    "    \n",
    "    # Apply obstacle wrapper if requested\n",
    "    if use_obstacles:\n",
    "        env = ObstacleBipedalWrapper(env, difficulty=obstacle_difficulty)\n",
    "\n",
    "    # Wrap environment to record video\n",
    "    # We force record the first episode\n",
    "    video_folder = \"videos\"\n",
    "    os.makedirs(video_folder, exist_ok=True)\n",
    "    env = RecordVideo(env, video_folder=video_folder, name_prefix=filename, episode_trigger=lambda x: True)\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    truncated = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not (done or truncated):\n",
    "        # Use deterministic policy for evaluation\n",
    "        action = agent.select_action(state, deterministic=True)\n",
    "\n",
    "        # Ensure action is 1D if it comes back as 2D batch (handle legacy/stale agent instances)\n",
    "        if isinstance(action, np.ndarray) and action.ndim > 1:\n",
    "             action = action.flatten()\n",
    "\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Evaluation Run - Total Reward: {total_reward:.2f}\")\n",
    "\n",
    "    # Find the video file\n",
    "    mp4_files = glob.glob(f\"{video_folder}/{filename}-episode-0.mp4\")\n",
    "    if mp4_files:\n",
    "        print(f\"Video saved to {mp4_files[0]}\")\n",
    "        return mp4_files[0]\n",
    "    return None\n",
    "\n",
    "# Find the best checkpoint across all log directories\n",
    "log_dirs = glob.glob(\"logs/run_*\")\n",
    "best_checkpoint_path = None\n",
    "best_reward = -float('inf')\n",
    "best_log_dir = None\n",
    "\n",
    "print(\"Searching through all log directories for best performance...\")\n",
    "\n",
    "if log_dirs:\n",
    "    for log_dir in log_dirs:\n",
    "        # Find training log file to get final reward metrics\n",
    "        log_files = glob.glob(f\"{log_dir}/training_*.log\")\n",
    "        if log_files:\n",
    "            # Try to extract best reward from log file\n",
    "            log_file = log_files[0]\n",
    "            try:\n",
    "                with open(log_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    # Look for lines with \"Avg Reward\" or similar metrics\n",
    "                    for line in reversed(lines):  # Check from end for latest stats\n",
    "                        if \"Avg Reward\" in line or \"Average Reward\" in line:\n",
    "                            # Extract the reward value\n",
    "                            import re\n",
    "                            numbers = re.findall(r\"[-+]?\\d*\\.?\\d+\", line)\n",
    "                            if numbers:\n",
    "                                try:\n",
    "                                    reward_value = float(numbers[-1])  # Usually the last number is the reward\n",
    "                                    if reward_value > best_reward:\n",
    "                                        best_reward = reward_value\n",
    "                                        best_log_dir = log_dir\n",
    "                                    break\n",
    "                                except:\n",
    "                                    continue\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Also check checkpoints directly for episode numbers\n",
    "        checkpoints = glob.glob(f\"{log_dir}/checkpoint_ep*.pth\")\n",
    "        if checkpoints:\n",
    "            def _ep_num(path):\n",
    "                name = os.path.basename(path)\n",
    "                try:\n",
    "                    return int(name.split(\"checkpoint_ep\")[1].split(\".pth\")[0])\n",
    "                except Exception:\n",
    "                    return -1\n",
    "\n",
    "            latest_checkpoint = max(checkpoints, key=_ep_num)\n",
    "            ep_num = _ep_num(latest_checkpoint)\n",
    "            print(f\"  {log_dir}: Latest episode {ep_num}\")\n",
    "\n",
    "# If we found a best log directory, use its highest episode checkpoint\n",
    "if best_log_dir:\n",
    "    checkpoints = glob.glob(f\"{best_log_dir}/checkpoint_ep*.pth\")\n",
    "    if checkpoints:\n",
    "        def _ep_num(path):\n",
    "            name = os.path.basename(path)\n",
    "            try:\n",
    "                return int(name.split(\"checkpoint_ep\")[1].split(\".pth\")[0])\n",
    "            except Exception:\n",
    "                return -1\n",
    "\n",
    "        best_checkpoint_path = max(checkpoints, key=_ep_num)\n",
    "        print(f\"\\n Found best performing run: {best_log_dir}\")\n",
    "        print(f\"  Using checkpoint from episode: {_ep_num(best_checkpoint_path)}\")\n",
    "else:\n",
    "    # Fallback: just use latest checkpoint from most recent log\n",
    "    if log_dirs:\n",
    "        latest_log_dir = max(log_dirs, key=os.path.getctime)\n",
    "        checkpoints = glob.glob(f\"{latest_log_dir}/checkpoint_ep*.pth\")\n",
    "        if checkpoints:\n",
    "            def _ep_num(path):\n",
    "                name = os.path.basename(path)\n",
    "                try:\n",
    "                    return int(name.split(\"checkpoint_ep\")[1].split(\".pth\")[0])\n",
    "                except Exception:\n",
    "                    return -1\n",
    "            best_checkpoint_path = max(checkpoints, key=_ep_num)\n",
    "            best_log_dir = latest_log_dir\n",
    "            print(f\"Using latest log directory: {latest_log_dir}\")\n",
    "\n",
    "if best_checkpoint_path:\n",
    "    print(f\"Loading agent from: {best_checkpoint_path}\")\n",
    "\n",
    "    # Create a fresh agent and load the checkpoint\n",
    "    env_temp = gym.make(\"BipedalWalker-v3\")\n",
    "    state_dim = env_temp.observation_space.shape[0]\n",
    "    action_dim = env_temp.action_space.shape[0]\n",
    "    env_temp.close()\n",
    "\n",
    "    checkpoint_agent = load_checkpoint(best_checkpoint_path, state_dim, action_dim, device=device)\n",
    "\n",
    "    # Use log directory name as video filename\n",
    "    log_dir_name = os.path.basename(best_log_dir)\n",
    "\n",
    "    # Record video with the loaded agent\n",
    "    # Check if the agent was trained with obstacles by looking at the log directory name\n",
    "    use_obstacles = \"with_obstacles\" in log_dir_name or \"_obs1\" in log_dir_name\n",
    "    obstacle_difficulty = 0.7 if use_obstacles else 0.0\n",
    "    \n",
    "    print(f\"Recording video with obstacles={use_obstacles}, difficulty={obstacle_difficulty}\")\n",
    "    video_path = record_video(checkpoint_agent, filename=log_dir_name, device=device, \n",
    "                               use_obstacles=use_obstacles, obstacle_difficulty=obstacle_difficulty)\n",
    "    if video_path:\n",
    "        display(Video(video_path, embed=True, html_attributes=\"controls autoplay loop\"))\n",
    "else:\n",
    "    print(\"No log directories or checkpoints found. Train the agent first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Runner\n",
    "Set up a lightweight sweep loop to vary seeds, entropy targets, learning rate, update ratio, tau, batch size, buffer size, hidden width, env count, and obstacle difficulty. Commented execution call lets you start/stop the sweep easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Build a grid of hyperparameters to sweep\n",
    "def build_ablation_grid():\n",
    "    # Primary study dimensions (3 LRs  2 entropy targets = 6 total runs)\n",
    "    learning_rates = [3e-4, 1e-4, 3e-5]  # For future: add more rates\n",
    "    entropy_targets = [-1.0, -4.0]  # Exploration strength; For future: [-1, -2, -3]\n",
    "    \n",
    "    # Fixed parameters (single value each - not currently being studied)\n",
    "    seeds = [42]  # Fixed seed for ablation; use build_seed_study_grid() for seed variance\n",
    "    update_ratios = [1]  # Gradient steps per env step; For future: [1, 2, 4]\n",
    "    taus = [0.005]  # Target network update rate; For future: [0.005, 0.01, 0.02]\n",
    "    batch_sizes = [1024]  # For future: [256, 512, 1024]\n",
    "    buffer_sizes = [int(1e6)]  # For future: [5e5, 1e6]\n",
    "    num_envs_list = [32]  # Parallel environments; For future: [8, 16, 32]\n",
    "    obstacle_settings = [(False, 0.0)]  # For future: [(False, 0.0), (True, 0.3), (True, 0.7)]\n",
    "    hidden_dims = [256]  # Network width; For future: [256, 512]\n",
    "\n",
    "    grid = []\n",
    "    for lr, ent_t, seed, upd, tau, bs, buf, num_envs, (use_obs, obs_d), hdim in product(\n",
    "        learning_rates, entropy_targets, seeds, update_ratios, taus, batch_sizes, buffer_sizes, num_envs_list, obstacle_settings, hidden_dims\n",
    "    ):\n",
    "        grid.append({\n",
    "            \"seed\": seed,\n",
    "            \"target_entropy\": ent_t,\n",
    "            \"learning_rate\": lr,\n",
    "            \"updates_per_step\": upd,\n",
    "            \"tau\": tau,\n",
    "            \"batch_size\": bs,\n",
    "            \"buffer_size\": buf,\n",
    "            \"num_envs\": num_envs,\n",
    "            \"use_obstacles\": use_obs,\n",
    "            \"obstacle_difficulty\": obs_d,\n",
    "            \"hidden_dim\": hdim,\n",
    "        })\n",
    "    return grid\n",
    "\n",
    "# Build a seed study grid using the best hyperparameters from ablation\n",
    "def build_seed_study_grid(best_lr=1e-4, best_entropy=-1.0):\n",
    "    \"\"\"Run the same config with multiple seeds to measure variance.\"\"\"\n",
    "    seeds = [42, 123, 789]  # Multiple seeds for reproducibility study\n",
    "    \n",
    "    grid = []\n",
    "    for seed in seeds:\n",
    "        grid.append({\n",
    "            \"seed\": seed,\n",
    "            \"target_entropy\": best_entropy,\n",
    "            \"learning_rate\": best_lr,\n",
    "            \"updates_per_step\": 1,\n",
    "            \"tau\": 0.005,\n",
    "            \"batch_size\": 1024,\n",
    "            \"buffer_size\": int(1e6),\n",
    "            \"num_envs\": 32,\n",
    "            \"use_obstacles\": False,\n",
    "            \"obstacle_difficulty\": 0.0,\n",
    "            \"hidden_dim\": 256,\n",
    "        })\n",
    "    return grid\n",
    "\n",
    "def _tag_from_cfg(cfg):\n",
    "    return (\n",
    "        f\"seed{cfg['seed']}_lr{cfg['learning_rate']}_ent{cfg['target_entropy']}\"\n",
    "        f\"_upd{cfg['updates_per_step']}_tau{cfg['tau']}_bs{cfg['batch_size']}\"\n",
    "        f\"_buf{cfg['buffer_size']}_env{cfg['num_envs']}_obs{int(cfg['use_obstacles'])}d{cfg['obstacle_difficulty']}\"\n",
    "        f\"_h{cfg['hidden_dim']}_t{int(time.time())}\"\n",
    "    )\n",
    "\n",
    "def run_ablation(grid, base_config, results_path=\"logs/ablations/results.json\"):\n",
    "    results = []\n",
    "    os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "\n",
    "    for idx, cfg in enumerate(grid):\n",
    "        run_cfg = {**base_config, **cfg}\n",
    "        run_tag = _tag_from_cfg(cfg)\n",
    "        run_cfg[\"log_dir\"] = os.path.join(\"logs\", \"ablations\", run_tag)\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"[Ablation {idx+1}/{len(grid)}] {run_tag}\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        rewards, _ = train_agent(**run_cfg)\n",
    "        mean_last_50 = float(np.mean(rewards[-50:])) if len(rewards) else 0.0\n",
    "        results.append({\n",
    "            **cfg,\n",
    "            \"episodes\": len(rewards),\n",
    "            \"mean_last_50\": mean_last_50,\n",
    "            \"log_dir\": run_cfg[\"log_dir\"],\n",
    "        })\n",
    "\n",
    "        # Save running results to disk so partial sweeps are recoverable\n",
    "        with open(results_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Base config shared across all sweeps (500-1000 episodes per run)\n",
    "base_ablation_cfg = dict(\n",
    "    env_name=\"BipedalWalker-v3\",\n",
    "    max_episodes=1000,  # Adjust to 1000 if needed\n",
    "    max_steps=1000,\n",
    "    device=device,\n",
    "    render_freq=0,\n",
    "    start_steps=5000,\n",
    "    save_interval=100,\n",
    ")\n",
    "\n",
    "# Build grid; comment/uncomment to launch\n",
    "ablation_grid = build_ablation_grid()\n",
    "print(f\"Planned ablation runs: {len(ablation_grid)} total configurations\")\n",
    "print(\"Studying 2 dimensions: learning_rate (3), entropy_target (2) = 6 runs\")\n",
    "\n",
    "# To run the full sweep, uncomment the line below:\n",
    "# ablation_results = run_ablation(ablation_grid, base_ablation_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation Results Visualization\n",
    "\n",
    "Visualize the ablation study results across seeds, learning rates, and entropy targets. Load results from the saved JSON and generate comparison plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load ablation results from JSON\n",
    "results_path = \"logs/ablations/results.json\"\n",
    "\n",
    "try:\n",
    "    with open(results_path, 'r') as f:\n",
    "        ablation_results = json.load(f)\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df = pd.DataFrame(ablation_results)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} ablation runs from {results_path}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nSummary statistics:\")\n",
    "    print(df[['seed', 'learning_rate', 'target_entropy', 'mean_last_50']].describe())\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Plot 1: Bar plot by learning rate\n",
    "    ax1 = axes[0]\n",
    "    lr_summary = df.groupby('learning_rate')['mean_last_50'].agg(['mean', 'std']).reset_index()\n",
    "    lr_summary['learning_rate_str'] = lr_summary['learning_rate'].apply(lambda x: f\"{x:.0e}\")\n",
    "    ax1.bar(lr_summary['learning_rate_str'], lr_summary['mean'], \n",
    "            yerr=lr_summary['std'], capsize=5, alpha=0.7, color='steelblue')\n",
    "    ax1.set_title('Performance by Learning Rate', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Learning Rate')\n",
    "    ax1.set_ylabel('Mean Reward (Last 50 Episodes)')\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Bar plot by entropy target\n",
    "    ax2 = axes[1]\n",
    "    ent_summary = df.groupby('target_entropy')['mean_last_50'].agg(['mean', 'std']).reset_index()\n",
    "    ent_summary['target_entropy_str'] = ent_summary['target_entropy'].astype(str)\n",
    "    ax2.bar(ent_summary['target_entropy_str'], ent_summary['mean'], \n",
    "            yerr=ent_summary['std'], capsize=5, alpha=0.7, color='coral')\n",
    "    ax2.set_title('Performance by Entropy Target', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Entropy Target (Exploration Strength)')\n",
    "    ax2.set_ylabel('Mean Reward (Last 50 Episodes)')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 3: Heatmap (learning rate  entropy target)\n",
    "    ax3 = axes[2]\n",
    "    pivot_table = df.pivot_table(values='mean_last_50', \n",
    "                                  index='learning_rate', \n",
    "                                  columns='target_entropy', \n",
    "                                  aggfunc='mean')\n",
    "    sns.heatmap(pivot_table, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax3, cbar_kws={'label': 'Mean Reward'})\n",
    "    ax3.set_title('Interaction: LR  Entropy', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Entropy Target')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    # Format y-axis labels\n",
    "    ax3.set_yticklabels([f\"{float(label.get_text()):.0e}\" for label in ax3.get_yticklabels()], rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('logs/ablations/ablation_results.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to logs/ablations/ablation_results.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best configuration\n",
    "    best_idx = df['mean_last_50'].idxmax()\n",
    "    best_config = df.loc[best_idx]\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BEST CONFIGURATION:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Learning Rate: {best_config['learning_rate']:.0e}\")\n",
    "    print(f\"Entropy Target: {best_config['target_entropy']}\")\n",
    "    print(f\"Mean Reward (Last 50 eps): {best_config['mean_last_50']:.2f}\")\n",
    "    print(f\"Log Directory: {best_config['log_dir']}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Print summary table grouped by hyperparameters\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY BY HYPERPARAMETER:\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Group by learning rate\n",
    "    lr_summary = df.groupby('learning_rate')['mean_last_50'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    print(\"\\nLearning Rate:\")\n",
    "    for lr, row in lr_summary.iterrows():\n",
    "        print(f\"  {lr:.0e}: {row['mean']:.2f}  {row['std']:.2f} [{row['min']:.2f}, {row['max']:.2f}] (n={int(row['count'])})\")\n",
    "    \n",
    "    # Group by entropy target\n",
    "    ent_summary = df.groupby('target_entropy')['mean_last_50'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "    print(\"\\nEntropy Target:\")\n",
    "    for ent, row in ent_summary.iterrows():\n",
    "        print(f\"  {ent:.1f}: {row['mean']:.2f}  {row['std']:.2f} [{row['min']:.2f}, {row['max']:.2f}] (n={int(row['count'])})\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Results file not found: {results_path}\")\n",
    "    print(\"Run the ablation study first by uncommenting the execution line in the previous cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or visualizing results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed Study Execution\n",
    "\n",
    "Run the same configuration with multiple seeds to measure variance and robustness. Use the best hyperparameters identified from the ablation study above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed study: Run with best hyperparameters across multiple seeds\n",
    "seed_study_grid = build_seed_study_grid(best_lr=1e-4, best_entropy=-1.0)\n",
    "print(f\"Planned seed study runs: {len(seed_study_grid)} configurations\")\n",
    "print(\"Studying seed variance with fixed hyperparameters\")\n",
    "\n",
    "# To run the seed study, uncomment the line below:\n",
    "# seed_results = run_ablation(seed_study_grid, base_ablation_cfg, results_path=\"logs/ablations/seed_study.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seed Study Visualization\n",
    "\n",
    "Visualize seed variance using the best hyperparameters from the ablation study. This measures the robustness of the configuration across different random seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load seed study results from JSON\n",
    "seed_results_path = \"logs/ablations/seed_study.json\"\n",
    "\n",
    "try:\n",
    "    with open(seed_results_path, 'r') as f:\n",
    "        seed_data = json.load(f)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    seed_df = pd.DataFrame(seed_data)\n",
    "    \n",
    "    print(f\"Loaded {len(seed_df)} seed study runs from {seed_results_path}\")\n",
    "    print(f\"\\nColumns: {seed_df.columns.tolist()}\")\n",
    "    \n",
    "    # Calculate statistics\n",
    "    seed_summary = seed_df.groupby('seed')['mean_last_50'].agg(['mean']).reset_index()\n",
    "    overall_mean = seed_df['mean_last_50'].mean()\n",
    "    overall_std = seed_df['mean_last_50'].std()\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot 1: Bar plot by seed\n",
    "    ax1 = axes[0]\n",
    "    seed_summary['seed_str'] = seed_summary['seed'].astype(str)\n",
    "    ax1.bar(seed_summary['seed_str'], seed_summary['mean'], alpha=0.7, color='green')\n",
    "    ax1.axhline(y=overall_mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {overall_mean:.2f}')\n",
    "    ax1.axhline(y=overall_mean + overall_std, color='orange', linestyle=':', linewidth=1, alpha=0.7, label=f'1 Std: {overall_std:.2f}')\n",
    "    ax1.axhline(y=overall_mean - overall_std, color='orange', linestyle=':', linewidth=1, alpha=0.7)\n",
    "    ax1.set_title('Performance by Seed', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Random Seed')\n",
    "    ax1.set_ylabel('Mean Reward (Last 50 Episodes)')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Box plot\n",
    "    ax2 = axes[1]\n",
    "    ax2.boxplot([seed_df['mean_last_50']], labels=['All Seeds'])\n",
    "    ax2.set_title('Seed Variance Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel('Mean Reward (Last 50 Episodes)')\n",
    "    ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('logs/ablations/seed_study_results.png', dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\nVisualization saved to logs/ablations/seed_study_results.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print seed study summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SEED STUDY RESULTS:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nOverall Performance: {overall_mean:.2f}  {overall_std:.2f}\")\n",
    "    print(\"\\nBy Seed:\")\n",
    "    for _, row in seed_summary.iterrows():\n",
    "        print(f\"  Seed {int(row['seed'])}: {row['mean']:.2f}\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Seed study results not found: {seed_results_path}\")\n",
    "    print(\"Run the seed study first by uncommenting the execution line in the ablation cell.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or visualizing seed study results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backup the logs and videos after training.\n",
    "Used only for colab execution to save logs and videos to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount your Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Define your paths\n",
    "source_folders = {\n",
    "    '/content/logs': '/content/drive/MyDrive/log/logs',\n",
    "    '/content/videos': '/content/drive/MyDrive/log/videos'\n",
    "}\n",
    "\n",
    "# 3. Execute the transfer\n",
    "for src, dest in source_folders.items():\n",
    "    if os.path.exists(src):\n",
    "        shutil.copytree(src, dest, dirs_exist_ok=True)\n",
    "        print(f\" Successfully synced: {src} -> {dest}\")\n",
    "    else:\n",
    "        print(f\" Source not found, skipping: {src}\")\n",
    "\n",
    "print(\"\\nBackup complete. You can view your files in the 'log' folder of your Drive.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
